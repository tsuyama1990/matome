import marimo

__generated_with = "0.1.0"
app = marimo.App()


@app.cell
def _():
    import logging
    import os
    import sys
    import json
    import random
    from pathlib import Path
    from typing import Any, List, Optional
    from unittest.mock import MagicMock, patch

    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.decomposition import PCA

    # Ensure src is in path
    project_root = Path.cwd()
    if str(project_root / "src") not in sys.path:
        sys.path.append(str(project_root / "src"))

    # Matome Imports
    from domain_models.config import ProcessingConfig
    from domain_models.manifest import Chunk, SummaryNode, DocumentTree, Cluster
    from domain_models.types import DIKWLevel
    from matome.engines.raptor import RaptorEngine
    from matome.engines.semantic_chunker import JapaneseSemanticChunker
    from matome.engines.embedder import EmbeddingService
    from matome.engines.cluster import GMMClusterer
    from matome.agents.summarizer import SummarizationAgent
    from matome.utils.store import DiskChunkStore
    from matome.utils.traversal import traverse_source_chunks
    from matome.exporters.markdown import export_to_markdown
    from matome.exporters.obsidian import ObsidianCanvasExporter

    # Configure Logging
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    logger = logging.getLogger("UAT")
    return (
        Any,
        Chunk,
        Cluster,
        DIKWLevel,
        DiskChunkStore,
        DocumentTree,
        EmbeddingService,
        GMMClusterer,
        JapaneseSemanticChunker,
        List,
        MagicMock,
        ObsidianCanvasExporter,
        Optional,
        Path,
        ProcessingConfig,
        RaptorEngine,
        SummarizationAgent,
        SummaryNode,
        export_to_markdown,
        json,
        logger,
        logging,
        os,
        patch,
        project_root,
        sys,
        traverse_source_chunks,
        np,
        plt,
        PCA,
        random
    )


@app.cell
def _(os, logger):
    # Check for API Key
    api_key = os.environ.get("OPENROUTER_API_KEY")
    is_mock_mode = not api_key

    if is_mock_mode:
        logger.info("⚠️  OPENROUTER_API_KEY not found. Running in MOCK MODE.")
    else:
        logger.info("✅  OPENROUTER_API_KEY found. Running in REAL MODE.")
    return api_key, is_mock_mode


@app.cell
def _(MagicMock, patch, is_mock_mode, logger, np):
    # Mocking Utilities

    def get_mock_embeddings(texts: list[str]):
        # Return numpy array of shape (len(texts), 384)
        return np.array([np.random.rand(384) for _ in texts])

    def mock_llm_call(*args, **kwargs):
        return "This is a mock summary generated by the UAT script."

    context_managers = []

    if is_mock_mode:
        # Mock Embeddings
        # We patch SentenceTransformer within EmbeddingService if possible, or higher level.

        # Mock the SentenceTransformer class itself where it's imported in embedder
        mock_model = MagicMock()
        mock_model.encode.side_effect = lambda texts, **kwargs: get_mock_embeddings(texts)

        p1 = patch("matome.engines.embedder.SentenceTransformer", return_value=mock_model)

        # Mock LLM
        # Patch the SummarizationAgent.summarize method
        p2 = patch("matome.agents.summarizer.SummarizationAgent.summarize", return_value="Mock Summary Content")

        # Also need to mock ChatOpenAI if it's instantiated
        p3 = patch("matome.agents.summarizer.ChatOpenAI", new=MagicMock())

        context_managers.extend([p1, p2, p3])
        for _cm in context_managers:
            _cm.start()

        logger.info("Mocks activated.")

    return context_managers, get_mock_embeddings, mock_llm_call


@app.cell
def _(logger, project_root, os):
    # Data Generation Logic
    test_data_dir = project_root / "test_data"
    if not test_data_dir.exists():
        os.makedirs(test_data_dir)
        logger.info(f"Created directory: {test_data_dir}")

    sample_txt_path = test_data_dir / "sample.txt"
    if not sample_txt_path.exists():
        dummy_text = "これはテスト用の日本語テキストです。\n" * 100
        with open(sample_txt_path, "w", encoding="utf-8") as _f_gen:
            _f_gen.write(dummy_text)
        logger.info(f"Created dummy file: {sample_txt_path}")

    return sample_txt_path, test_data_dir


@app.cell
def _(logger, sample_txt_path):
    # Scenario 1: Quickstart (Data Loading)
    logger.info("--- Scenario 1: Quickstart ---")

    # Load Text
    with open(sample_txt_path, "r", encoding="utf-8") as _f_sc1:
        content_sample = _f_sc1.read()

    logger.info(f"Loaded {len(content_sample)} chars from {sample_txt_path.name}")
    return content_sample,


@app.cell
def _(
    EmbeddingService,
    JapaneseSemanticChunker,
    ProcessingConfig,
    content_sample,
    logger,
):
    # Scenario 1 Continued: Chunking Visualization

    # Initialize basic config for chunking
    config_chunking = ProcessingConfig(
        embedding_model="sentence-transformers/all-MiniLM-L6-v2",
        chunk_buffer_size=10,
    )

    # We need an embedder for semantic chunking
    embedder_chunking = EmbeddingService(config_chunking)
    chunker_simple = JapaneseSemanticChunker(embedder_chunking)

    logger.info("Splitting text into chunks...")
    chunks_sample = list(chunker_simple.split_text(content_sample, config_chunking))

    logger.info(f"Generated {len(chunks_sample)} chunks.")

    print("--- First 5 Chunks ---")
    for i, c in enumerate(chunks_sample[:5]):
        print(f"[{c.index}] {c.text[:50]}...")

    return chunker_simple, chunks_sample, config_chunking, embedder_chunking


@app.cell
def _(chunks_sample, embedder_chunking, logger, np, PCA, plt, project_root):
    # Scenario 2: Clustering Deep Dive (Visualization)
    logger.info("--- Scenario 2: Clustering Deep Dive ---")

    # Generate embeddings for the chunks
    texts = [c.text for c in chunks_sample]
    embeddings = list(embedder_chunking.embed_strings(texts))
    embeddings_np = np.array(embeddings)

    logger.info(f"Generated embeddings shape: {embeddings_np.shape}")

    # Reduce dimensions to 2D
    pca = PCA(n_components=2)
    reduced_embeddings = pca.fit_transform(embeddings_np)

    # Plot
    plt.figure(figsize=(10, 6))
    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.7)
    plt.title("Chunk Embeddings Visualization (PCA)")
    plt.xlabel("Component 1")
    plt.ylabel("Component 2")
    plt.grid(True)

    plot_path = project_root / "clustering_plot.png"
    plt.savefig(plot_path)
    logger.info(f"Saved clustering plot to {plot_path}")

    return embeddings_np, plot_path, reduced_embeddings, pca, texts


@app.cell
def _(
    DiskChunkStore,
    EmbeddingService,
    GMMClusterer,
    JapaneseSemanticChunker,
    ProcessingConfig,
    RaptorEngine,
    SummarizationAgent,
    SummaryNode,
    logger,
    sample_txt_path,
    test_data_dir
):
    # Scenario 3: Full Raptor Pipeline
    logger.info("--- Scenario 3: Full Raptor Pipeline ---")

    # Determine input file
    target_file = test_data_dir / "エミン流「会社四季報」最強の読み方.txt"
    if not target_file.exists():
        logger.warning(f"Target file {target_file.name} not found. Using {sample_txt_path.name} instead.")
        target_file = sample_txt_path

    with open(target_file, "r", encoding="utf-8") as _f_sc3:
        full_content = _f_sc3.read()

    # Initialize Full Engine
    # We use a temporary store for the tutorial
    store = DiskChunkStore(db_path=None) # In-memory/Temp file

    config = ProcessingConfig(
        n_clusters=2,
        umap_n_neighbors=2,
        embedding_model="sentence-transformers/all-MiniLM-L6-v2",
        summarization_model="openai/gpt-4o-mini",
        chunk_buffer_size=10,
    )

    embedder = EmbeddingService(config)
    chunker = JapaneseSemanticChunker(embedder)
    clusterer = GMMClusterer()
    summarizer = SummarizationAgent(config)

    engine = RaptorEngine(
        chunker=chunker,
        embedder=embedder,
        clusterer=clusterer,
        summarizer=summarizer,
        config=config
    )

    logger.info("Running Raptor Engine...")
    try:
        tree = engine.run(full_content, store=store)
        root_id = tree.root_node.id if isinstance(tree.root_node, SummaryNode) else str(tree.root_node.index)
        logger.info(f"Pipeline complete. Root Node ID: {root_id}")
    except Exception as e:
        logger.error(f"Pipeline failed: {e}")
        store.close()
        raise

    return config, embedder, chunker, clusterer, summarizer, engine, tree, store, root_id


@app.cell
def _(export_to_markdown, project_root, store, tree, logger):
    # Scenario 3 Continued: Markdown Export
    md_content = export_to_markdown(tree, store)

    output_md_path = project_root / "summary_all.md"
    with open(output_md_path, "w", encoding="utf-8") as _f_md:
        _f_md.write(md_content)
    logger.info(f"Exported Markdown to {output_md_path}")
    return md_content, output_md_path


@app.cell
def _(ObsidianCanvasExporter, project_root, store, tree, logger):
    # Scenario 4: KJ Method Visualization (Canvas Export)
    logger.info("--- Scenario 4: Export to Obsidian Canvas ---")

    canvas_exporter = ObsidianCanvasExporter()

    output_canvas_path = project_root / "summary_kj.canvas"

    # Use export method which writes to file
    canvas_exporter.export(tree, output_canvas_path, store)

    logger.info(f"Exported Canvas to {output_canvas_path}")
    return canvas_exporter, output_canvas_path


@app.cell
def _(context_managers, logger, store):
    # Cleanup
    store.close()
    for _cm in context_managers:
        _cm.stop()
    logger.info("Mocks deactivated.")
    logger.info("✅ UAT COMPLETE")
    return


if __name__ == "__main__":
    app.run()
