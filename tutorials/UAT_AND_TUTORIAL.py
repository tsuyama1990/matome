import marimo

__generated_with = "0.10.14"
app = marimo.App(width="medium")


@app.cell
def _():
    import logging
    import os
    import sys
    import tempfile
    import uuid
    from pathlib import Path
    from typing import Any, Iterator, cast

    import matplotlib.pyplot as plt
    import numpy as np
    from domain_models.config import ProcessingConfig
    from domain_models.types import DIKWLevel
    from domain_models.manifest import Chunk, SummaryNode
    from matome.agents.summarizer import SummarizationAgent
    from matome.engines.cluster import GMMClusterer
    from matome.engines.embedder import EmbeddingService
    from matome.engines.interactive_raptor import InteractiveRaptorEngine
    from matome.engines.raptor import RaptorEngine
    from matome.engines.token_chunker import JapaneseTokenChunker
    from matome.interfaces import PromptStrategy
    from matome.utils.store import DiskChunkStore
    from matome.exporters.markdown import export_to_markdown
    from matome.exporters.obsidian import ObsidianCanvasExporter

    # Setup logging
    logging.basicConfig(stream=sys.stdout, level=logging.INFO, force=True)
    logger = logging.getLogger("matome.tutorial")
    return (
        Any,
        Chunk,
        DIKWLevel,
        DiskChunkStore,
        EmbeddingService,
        GMMClusterer,
        InteractiveRaptorEngine,
        Iterator,
        JapaneseTokenChunker,
        ObsidianCanvasExporter,
        Path,
        ProcessingConfig,
        PromptStrategy,
        RaptorEngine,
        SummarizationAgent,
        SummaryNode,
        cast,
        export_to_markdown,
        logger,
        logging,
        np,
        os,
        plt,
        sys,
        tempfile,
        uuid,
    )


@app.cell
def _(Any, EmbeddingService, Iterator, ProcessingConfig, PromptStrategy, SummarizationAgent, logger, np, uuid):
    # --- MOCK CLASSES ---

    class MockEmbeddingService(EmbeddingService):
        """Mock Embedder to avoid downloading heavy models."""

        def __init__(self, config: ProcessingConfig):
            super().__init__(config)
            self.dim = 384  # Default for all-MiniLM-L6-v2

        def embed_strings(self, texts: Any) -> Iterator[list[float]]:
            # Return random vectors
            for _ in texts:
                # Use seed for deterministic results
                yield np.random.rand(self.dim).tolist()

        def embed_chunks(self, chunks: Iterator[Chunk]) -> Iterator[Chunk]:
            for chunk in chunks:
                chunk.embedding = np.random.rand(self.dim).tolist()
                yield chunk

    class MockSummarizationAgent(SummarizationAgent):
        """Mock Agent to return deterministic summaries based on strategy."""

        def summarize(
            self,
            text: str,
            config: ProcessingConfig | None = None,
            strategy: PromptStrategy | None = None,
            context: dict[str, Any] | None = None,
        ) -> str:
            # Check context for refinement instruction
            if context and "instruction" in context:
                return f"Refined: {context['instruction']} (Original len: {len(text)})"

            # Check strategy for DIKW level
            level_name = "Summary"
            if strategy:
                try:
                    level_name = strategy.target_dikw_level.value.capitalize()
                except AttributeError:
                    level_name = type(strategy).__name__

            return f"{level_name}: {text[:50]}... (Mock Generated)"
    return MockEmbeddingService, MockSummarizationAgent


@app.cell
def _(
    DiskChunkStore,
    EmbeddingService,
    GMMClusterer,
    JapaneseTokenChunker,
    MockEmbeddingService,
    MockSummarizationAgent,
    ProcessingConfig,
    RaptorEngine,
    SummarizationAgent,
    logger,
    os,
):
    # --- SETUP & INITIALIZATION ---

    # Determine mode
    api_key = os.environ.get("OPENROUTER_API_KEY")
    mock_mode = not api_key or api_key == "mock"

    logger.info(f"Running in {'MOCK' if mock_mode else 'REAL'} mode.")

    # Initialize Config
    config = ProcessingConfig()

    # Ensure tutorials directory exists
    os.makedirs("tutorials", exist_ok=True)

    # Initialize Components
    chunker = JapaneseTokenChunker(config)
    clusterer = GMMClusterer()

    if mock_mode:
        embedder = MockEmbeddingService(config)
        summarizer = MockSummarizationAgent(config)
    else:
        embedder = EmbeddingService(config)
        summarizer = SummarizationAgent(config)

    # Initialize Engine
    engine = RaptorEngine(chunker, embedder, clusterer, summarizer, config)
    return (
        api_key,
        chunker,
        clusterer,
        config,
        embedder,
        engine,
        mock_mode,
        summarizer,
    )


@app.cell
def _(chunker, config, logger):
    # --- SCENARIO 1: Quickstart (The Basics) ---
    logger.info("Starting SCENARIO 1: Quickstart")

    # Sample text (simulating a financial document)
    SAMPLE_TEXT = """
    ã€æŠ•è³‡å“²å­¦ã€‘
    ãƒãƒªãƒ¥ãƒ¼æŠ•è³‡ã¯ã€ä½•ã‚‰ã‹ã®ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«åˆ†æžã«ã‚ˆã‚ŠéŽå°è©•ä¾¡ã•ã‚Œã¦ã„ã‚‹ã¨æ€ã‚ã‚Œã‚‹è¨¼åˆ¸ã‚’è³¼å…¥ã™ã‚‹æŠ•è³‡æ‰‹æ³•ã§ã‚ã‚‹ã€‚
    ã“ã®æ¦‚å¿µã¯ãƒ™ãƒ³ã‚¸ãƒ£ãƒŸãƒ³ãƒ»ã‚°ãƒ¬ã‚¢ãƒ ã¨ãƒ‡ãƒ“ãƒƒãƒ‰ãƒ»ãƒ‰ãƒƒãƒ‰ã«ã‚ˆã£ã¦æœ€åˆã«åºƒã‚ã‚‰ã‚ŒãŸã€‚
    ã‚¦ã‚©ãƒ¼ãƒ¬ãƒ³ãƒ»ãƒãƒ•ã‚§ãƒƒãƒˆã¯ã“ã®æˆ¦ç•¥ã®æœ€ã‚‚æœ‰åãªæ”¯æŒè€…ã®ä¸€äººã§ã‚ã‚‹ã€‚

    ã€ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã€‘
    ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€è¡¨ç¾å­¦ç¿’ã‚’ä¼´ã†äººå·¥ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«åŸºã¥ãæ©Ÿæ¢°å­¦ç¿’æ‰‹æ³•ã®åºƒç¯„ãªãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ä¸€éƒ¨ã§ã‚ã‚‹ã€‚
    å­¦ç¿’ã¯ã€æ•™å¸«ã‚ã‚Šã€åŠæ•™å¸«ã‚ã‚Šã€ã¾ãŸã¯æ•™å¸«ãªã—ã§è¡Œã†ã“ã¨ãŒã§ãã‚‹ã€‚
    ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ“ãƒªãƒ¼ãƒ•ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€æ·±å±¤å¼·åŒ–å­¦ç¿’ã€ãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãªã©ã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã€éŸ³å£°èªè­˜ã€è‡ªç„¶è¨€èªžå‡¦ç†ã€æ©Ÿæ¢°ç¿»è¨³ã€ãƒã‚¤ã‚ªã‚¤ãƒ³ãƒ•ã‚©ãƒžãƒ†ã‚£ã‚¯ã‚¹ã€å‰µè–¬ã€åŒ»ç™‚ç”»åƒåˆ†æžã€ææ–™æ¤œæŸ»ã€ãƒœãƒ¼ãƒ‰ã‚²ãƒ¼ãƒ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ãªã©ã®åˆ†é‡Žã«é©ç”¨ã•ã‚Œã€äººé–“ã®å°‚é–€å®¶ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ã«åŒ¹æ•µã—ã€å ´åˆã«ã‚ˆã£ã¦ã¯ãã‚Œã‚’è¶…ãˆã‚‹çµæžœã‚’ç”Ÿã¿å‡ºã—ã¦ã„ã‚‹ã€‚

    ã€å››å­£å ±ã®èª­ã¿æ–¹ã€‘
    ä¼šç¤¾å››å­£å ±ã¯ã€æ—¥æœ¬ã®å…¨ä¸Šå ´ä¼æ¥­ã®ãƒ‡ãƒ¼ã‚¿ãƒ–ãƒƒã‚¯ã§ã‚ã‚‹ã€‚
    æ¥­ç¸¾äºˆæƒ³ã€è²¡å‹™çŠ¶æ³ã€æ ªä¸»æ§‹æˆãªã©ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã€‚
    ç‰¹ã«é‡è¦ãªã®ã¯ã€Œæ¥­ç¸¾æ¬„ã€ã§ã‚ã‚Šã€å£²ä¸Šé«˜ã‚„å–¶æ¥­åˆ©ç›Šã®æŽ¨ç§»ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã§ã€ä¼æ¥­ã®æˆé•·æ€§ã‚’åˆ¤æ–­ã§ãã‚‹ã€‚
    ã¾ãŸã€ã€Œææ–™æ¬„ã€ã«ã¯ã€æ–°è£½å“ã®é–‹ç™ºçŠ¶æ³ã‚„ææºè©±ãªã©ã€å°†æ¥ã®æ ªä¾¡ã«å½±éŸ¿ã‚’ä¸Žãˆã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹æƒ…å ±ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒå¤šã„ã€‚
    PERï¼ˆæ ªä¾¡åŽç›ŠçŽ‡ï¼‰ã‚„PBRï¼ˆæ ªä¾¡ç´”è³‡ç”£å€çŽ‡ï¼‰ãªã©ã®æŒ‡æ¨™ã‚‚é‡è¦ã§ã‚ã‚‹ãŒã€ã“ã‚Œã‚‰ã¯ã‚ãã¾ã§éŽåŽ»ã®å®Ÿç¸¾ã«åŸºã¥ãã‚‚ã®ã§ã‚ã‚Šã€å°†æ¥ã®æˆé•·æ€§ã‚’åŠ å‘³ã—ã¦åˆ¤æ–­ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
    """ * 3  # Duplicate to ensure enough content for chunking

    # 1. Chunking
    chunks = list(chunker.split_text(SAMPLE_TEXT, config))
    logger.info(f"Generated {len(chunks)} chunks.")

    # 2. Visualize Chunks
    print("--- First 5 Chunks ---")
    for i, chunk in enumerate(chunks[:5]):
        print(f"Chunk {i}: {chunk.text.strip()[:50]}...")

    assert len(chunks) > 0, "Should generate chunks"

    print("âœ… SCENARIO 1 Passed: Text ingested and chunked.")
    return SAMPLE_TEXT, chunks, i


@app.cell
def _(chunks, clusterer, embedder, logger, plt):
    # --- SCENARIO 2: Clustering Deep Dive (The Engine) ---
    logger.info("Starting SCENARIO 2: Clustering Deep Dive")

    # 1. Generate Embeddings
    # Note: embedder.embed_chunks yields chunks with embeddings populated
    embedded_chunks = list(embedder.embed_chunks(iter(chunks)))
    logger.info("Embeddings generated.")

    # 2. Run Clustering
    # We manually trigger clustering to visualize it
    # GMMClusterer expects chunks with embeddings
    # Note: GMMClusterer.cluster returns (clusters, global_embeddings)
    # But wait, RaptorEngine handles this. Let's inspect GMMClusterer.
    # We can just inspect the embeddings for visualization since clustering logic is internal

    # Extract embeddings for visualization
    embeddings = [c.embedding for c in embedded_chunks if c.embedding]

    if len(embeddings) >= 2:
        # Visualize 2D projection
        # We use PCA for simplicity here (mocking UMAP)
        from sklearn.decomposition import PCA

        pca = PCA(n_components=2)
        coords = pca.fit_transform(embeddings)

        plt.figure(figsize=(8, 6))
        plt.scatter(coords[:, 0], coords[:, 1], alpha=0.5)
        plt.title("Chunk Embeddings Projection (PCA)")
        plt.xlabel("Component 1")
        plt.ylabel("Component 2")
        plt.grid(True)
        # In a real notebook, this would show the plot. In headless, we save it.
        plt.savefig("tutorials/clustering_visualization.png")
        logger.info("Clustering visualization saved to tutorials/clustering_visualization.png")
    else:
        logger.warning("Not enough embeddings to visualize.")

    print("âœ… SCENARIO 2 Passed: Embeddings generated and visualized.")
    return embedded_chunks, embeddings


@app.cell
def _(DiskChunkStore, Path, SAMPLE_TEXT, engine, export_to_markdown, logger):
    # --- SCENARIO 3: Full Raptor Pipeline (The "Aha!" Moment) ---
    logger.info("Starting SCENARIO 3: Full Raptor Pipeline")

    # Setup temporary DB
    db_path = Path("tutorials/chunks.db")
    if db_path.exists():
        db_path.unlink()

    store = DiskChunkStore(db_path)

    # Run Engine
    try:
        root_tree = engine.run(SAMPLE_TEXT, store)
        logger.info("Raptor Engine finished successfully.")
    except Exception as e:
        logger.error(f"Raptor Engine failed: {e}")
        raise

    # Validation
    root_node = root_tree.root_node
    logger.info(f"Root Node Level: {root_node.level}")

    # Export to Markdown
    md_output = export_to_markdown(root_tree, store)
    with open("summary_all.md", "w", encoding="utf-8") as f:
        f.write(md_output)

    logger.info("Exported summary to summary_all.md")

    assert Path("summary_all.md").exists(), "Markdown file should exist"
    assert len(md_output) > 0, "Markdown content should not be empty"

    print("âœ… SCENARIO 3 Passed: Pipeline executed and Markdown exported.")
    return db_path, md_output, root_node, root_tree, store


@app.cell
def _(ObsidianCanvasExporter, Path, config, logger, root_tree, store):
    # --- SCENARIO 4: KJ Method Visualization (The Output) ---
    logger.info("Starting SCENARIO 4: KJ Method Visualization")

    exporter = ObsidianCanvasExporter(config)
    output_path = Path("summary_kj.canvas")

    exporter.export(root_tree, output_path, store)

    logger.info(f"Exported Canvas to {output_path}")

    assert output_path.exists(), "Canvas file should exist"

    print("âœ… SCENARIO 4 Passed: Obsidian Canvas exported.")
    return exporter, output_path


@app.cell
def _(
    Chunk,
    InteractiveRaptorEngine,
    config,
    root_node,
    store,
    summarizer,
):
    # --- BONUS: Interactive Refinement & Traceability ---

    # Interactive Refinement
    interactive = InteractiveRaptorEngine(store, summarizer, config)
    target_node_id = root_node.id
    instruction = "Explain like I'm 5"

    refined_node = interactive.refine_node(str(target_node_id), instruction)

    assert refined_node.metadata.is_user_edited is True
    assert instruction in refined_node.metadata.refinement_history

    print(f"Refined Node: {refined_node.text[:50]}...")
    print("âœ… Interactive Refinement Verified.")

    # Traceability
    source_chunks = list(interactive.get_source_chunks(str(target_node_id)))
    assert len(source_chunks) > 0
    assert isinstance(source_chunks[0], Chunk)

    print(f"Traced {len(source_chunks)} source chunks.")
    print("âœ… Traceability Verified.")
    return (
        instruction,
        interactive,
        refined_node,
        source_chunks,
        target_node_id,
    )


@app.cell
def _(db_path):
    print("ðŸŽ‰ All Scenarios Passed!")
    print(f"To explore the tree visually, run: uv run matome serve {db_path}")
    return


if __name__ == "__main__":
    app.run()
