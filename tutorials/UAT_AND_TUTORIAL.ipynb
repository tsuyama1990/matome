{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import marimo as mo\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Iterator, Iterable\n",
    "\n",
    "# Robust project root finding\n",
    "cwd = Path.cwd()\n",
    "if (cwd / \"src\").is_dir():\n",
    "    project_root = cwd\n",
    "elif (cwd.parent / \"src\").is_dir():\n",
    "    project_root = cwd.parent\n",
    "else:\n",
    "    # Fallback to assuming we are in tutorials/ and src is in ../src\n",
    "    project_root = cwd.parent\n",
    "\n",
    "if str(project_root / \"src\") not in sys.path:\n",
    "    sys.path.append(str(project_root / \"src\"))\n",
    "\n",
    "from domain_models.config import ProcessingConfig\n",
    "from domain_models.manifest import Chunk, SummaryNode\n",
    "from matome.engines.token_chunker import JapaneseTokenChunker\n",
    "from matome.engines.embedder import EmbeddingService\n",
    "from matome.engines.cluster import GMMClusterer\n",
    "from matome.engines.raptor import RaptorEngine\n",
    "from matome.agents.summarizer import SummarizationAgent\n",
    "from matome.exporters.obsidian import ObsidianCanvasExporter\n",
    "from matome.utils.store import DiskChunkStore\n",
    "\n",
    "# Setup directories\n",
    "test_data_dir = project_root / \"test_data\"\n",
    "test_data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "tutorials_dir = project_root / \"tutorials\"\n",
    "tutorials_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create dummy files if not exist in test_data\n",
    "sample_txt = test_data_dir / \"sample.txt\"\n",
    "if not sample_txt.exists():\n",
    "    sample_txt.write_text(\"これはサンプルのテキストです。Matome 2.0のテスト用です。\\n\\n二つ目の段落です。日本語のチャンキングをテストします。\")\n",
    "\n",
    "full_txt = test_data_dir / \"エミン流「会社四季報」最強の読み方.txt\"\n",
    "if not full_txt.exists():\n",
    "    # Create a longer dummy text\n",
    "    dummy_content = \"これは長いドキュメントのシミュレーションです。\\n\" * 100\n",
    "    full_txt.write_text(dummy_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJUe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine Mode\n",
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "mock_mode = not bool(api_key)\n",
    "\n",
    "mode_msg = \"Running in **MOCK MODE** (No API Key detected)\" if mock_mode else \"Running in **REAL MODE**\"\n",
    "mo.md(f\"\"\"\n",
    "# Matome 2.0 UAT & Tutorial\n",
    "\n",
    "{mode_msg}\n",
    "\n",
    "This notebook validates the core functionality of Matome 2.0:\n",
    "1.  **Quickstart**: Text Chunking\n",
    "2.  **Clustering**: Embedding & GMM\n",
    "3.  **Raptor Pipeline**: Recursive Summarization\n",
    "4.  **Visualization**: Obsidian Canvas Export\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock Classes definition\n",
    "\n",
    "class MockEmbeddingService(EmbeddingService):\n",
    "    \"\"\"Mock embedding service returning random vectors.\"\"\"\n",
    "    def __init__(self, config: ProcessingConfig):\n",
    "        super().__init__(config)\n",
    "        self.dim = 384 # Default dimension for all-MiniLM-L6-v2\n",
    "\n",
    "    def embed_strings(self, texts: Iterable[str]) -> Iterator[list[float]]:\n",
    "        for _ in texts:\n",
    "            # Return random vector\n",
    "            yield np.random.rand(self.dim).tolist()\n",
    "\n",
    "    def embed_chunks(self, chunks: Iterable[Chunk]) -> Iterator[Chunk]:\n",
    "        for chunk in chunks:\n",
    "            chunk.embedding = np.random.rand(self.dim).tolist()\n",
    "            yield chunk\n",
    "\n",
    "class MockSummarizationAgent(SummarizationAgent):\n",
    "    \"\"\"Mock summarization agent.\"\"\"\n",
    "    def __init__(self, config: ProcessingConfig):\n",
    "        # Bypass super init that checks API key\n",
    "        self.config = config\n",
    "        self.model_name = config.summarization_model\n",
    "        self.mock_mode = True\n",
    "        self.llm = None\n",
    "\n",
    "    def summarize(self, text, config=None, strategy=None, context=None):\n",
    "        return f\"Summary of: {text[:20]}...\"\n",
    "\n",
    "# Factory to get services\n",
    "def get_services(config: ProcessingConfig):\n",
    "    if mock_mode:\n",
    "        return MockEmbeddingService(config), MockSummarizationAgent(config)\n",
    "\n",
    "    # Real mode\n",
    "    # Note: EmbeddingService loads model which might be slow, but okay for Real mode UAT\n",
    "    return EmbeddingService(config), SummarizationAgent(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: Quickstart (Chunking)\n",
    "mo.md(\"## 1. Quickstart: Chunking\")\n",
    "\n",
    "config = ProcessingConfig()\n",
    "chunker = JapaneseTokenChunker(config)\n",
    "\n",
    "text = sample_txt.read_text()\n",
    "chunks = list(chunker.split_text(text, config))\n",
    "\n",
    "mo.md(f\"**Loaded {len(text)} chars.**\\n**Generated {len(chunks)} chunks.**\\n\\nFirst chunk: `{chunks[0].text if chunks else 'None'}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2: Clustering\n",
    "mo.md(\"## 2. Clustering Engine\")\n",
    "\n",
    "# We need embeddings first\n",
    "cluster_config = ProcessingConfig()\n",
    "embedder, _ = get_services(cluster_config)\n",
    "\n",
    "# Embed chunks (using generator to simulate streaming)\n",
    "# We iterate and collect to pass to clusterer which expects iterable of (id, vec)\n",
    "\n",
    "embedded_chunks = list(embedder.embed_chunks(chunks))\n",
    "\n",
    "# Prepare input for clusterer: Iterable[tuple[NodeID, list[float]]]\n",
    "# chunk.index is int, clusterer expects NodeID (str|int)\n",
    "embeddings_input = [(c.index, c.embedding) for c in embedded_chunks]\n",
    "\n",
    "clusterer = GMMClusterer()\n",
    "clusters = clusterer.cluster_nodes(embeddings_input, cluster_config)\n",
    "\n",
    "mo.md(f\"**Generated {len(clusters)} clusters.** from {len(embedded_chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 3: Raptor Pipeline\n",
    "mo.md(\"## 3. Raptor Pipeline\")\n",
    "\n",
    "raptor_config = ProcessingConfig()\n",
    "embedder_service, summarizer_service = get_services(raptor_config)\n",
    "\n",
    "engine = RaptorEngine(\n",
    "    chunker=chunker,\n",
    "    embedder=embedder_service,\n",
    "    clusterer=clusterer,\n",
    "    summarizer=summarizer_service,\n",
    "    config=raptor_config\n",
    ")\n",
    "\n",
    "text_content = full_txt.read_text()\n",
    "\n",
    "# Run Raptor with persistent Store in tutorials/ directory\n",
    "db_path = tutorials_dir / \"chunks.db\"\n",
    "if db_path.exists():\n",
    "    db_path.unlink()\n",
    "\n",
    "store = DiskChunkStore(db_path=db_path)\n",
    "\n",
    "try:\n",
    "    # Pass store to run() to ensure data is persisted in our DB\n",
    "    tree = engine.run(text_content, store=store)\n",
    "\n",
    "    summary_path = tutorials_dir / \"summary_all.md\"\n",
    "    summary_path.write_text(tree.root_node.text)\n",
    "\n",
    "    result_msg = f\"**Pipeline Complete!**\\nRoot Summary Length: {len(tree.root_node.text)}\\nSaved to `{summary_path}`\"\n",
    "except Exception as e:\n",
    "    result_msg = f\"**Pipeline Failed**: {e}\"\n",
    "    # Cleanup if failed\n",
    "    store.close()\n",
    "    raise e\n",
    "\n",
    "mo.md(result_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 4: Visualization\n",
    "mo.md(\"## 4. KJ Method Visualization (Obsidian Canvas)\")\n",
    "\n",
    "exporter = ObsidianCanvasExporter(raptor_config)\n",
    "output_path = tutorials_dir / \"summary_kj.canvas\"\n",
    "\n",
    "# Use the same store from previous step which contains the data\n",
    "exporter.export(tree, output_path, store)\n",
    "\n",
    "# Close store when done\n",
    "store.close()\n",
    "\n",
    "mo.md(f\"**Exported to `{output_path}`**\\n\\nYou can open this file in Obsidian.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "marimo": {
   "app_config": {
    "width": "medium"
   },
   "marimo_version": "0.19.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
