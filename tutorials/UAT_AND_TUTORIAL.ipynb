{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import uuid\n",
    "from collections.abc import Iterable, Iterator\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import marimo as mo\n",
    "\n",
    "# Add src to path if running from root\n",
    "if \"src\" not in sys.path:\n",
    "    sys.path.append(\"src\")\n",
    "\n",
    "from domain_models.config import ProcessingConfig\n",
    "from domain_models.manifest import Chunk, SummaryNode\n",
    "from domain_models.types import DIKWLevel\n",
    "from matome.agents.summarizer import SummarizationAgent\n",
    "from matome.engines.cluster import GMMClusterer\n",
    "from matome.engines.embedder import EmbeddingService\n",
    "from matome.engines.interactive_raptor import InteractiveRaptorEngine\n",
    "from matome.engines.raptor import RaptorEngine\n",
    "from matome.engines.token_chunker import JapaneseTokenChunker\n",
    "from matome.exporters.markdown import export_to_markdown\n",
    "from matome.exporters.obsidian import ObsidianCanvasExporter\n",
    "from matome.utils.store import DiskChunkStore\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(\"matome.tutorial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJUe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockEmbeddingService:\n",
    "    \"\"\"Mock embedding service that returns random vectors.\"\"\"\n",
    "\n",
    "    def __init__(self, config: ProcessingConfig) -> None:\n",
    "        self.config = config\n",
    "        # Use fixed seed for reproducibility in tests\n",
    "        np.random.seed(42)\n",
    "\n",
    "    def embed_strings(self, texts: Iterable[str]) -> Iterator[list[float]]:\n",
    "        \"\"\"Generate random embeddings for strings.\"\"\"\n",
    "        for _ in texts:\n",
    "            # Generate random vector of size 384 (standard for small models)\n",
    "            yield np.random.rand(384).tolist()\n",
    "\n",
    "    def embed_chunks(self, chunks: Iterable[Any]) -> Iterator[Any]:\n",
    "        \"\"\"Generate random embeddings for chunks.\"\"\"\n",
    "        for chunk in chunks:\n",
    "            chunk.embedding = np.random.rand(384).tolist()\n",
    "            yield chunk\n",
    "\n",
    "logger.info(\"MockEmbeddingService defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Configuration and Environment\n",
    "\n",
    "# 1. Setup DB Path\n",
    "db_path = \"tutorials/chunks.db\"\n",
    "if os.path.exists(db_path):\n",
    "    os.remove(db_path)\n",
    "if os.path.exists(f\"{db_path}-shm\"):\n",
    "    os.remove(f\"{db_path}-shm\")\n",
    "if os.path.exists(f\"{db_path}-wal\"):\n",
    "    os.remove(f\"{db_path}-wal\")\n",
    "\n",
    "# 2. Setup Config\n",
    "# Use 'mock' API key if not present to trigger mock mode in agents\n",
    "if not os.getenv(\"OPENAI_API_KEY\") and not os.getenv(\"OPENROUTER_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"mock\"\n",
    "    os.environ[\"OPENROUTER_API_KEY\"] = \"mock\"\n",
    "    logger.info(\"Running in MOCK MODE (No API Key found)\")\n",
    "else:\n",
    "    logger.info(\"Running in REAL MODE (API Key found)\")\n",
    "\n",
    "config = ProcessingConfig(\n",
    "    max_tokens=100,  # Small chunk size to force multiple chunks for small text\n",
    "    max_summary_tokens=50,\n",
    "    clustering_probability_threshold=0.1, # Low threshold to ensure clustering\n",
    "    umap_n_neighbors=2, # Small number for small dataset\n",
    "    umap_n_components=2,\n",
    "    umap_min_dist=0.0,\n",
    ")\n",
    "\n",
    "# 3. Initialize Components\n",
    "store = DiskChunkStore(db_path=Path(db_path))\n",
    "chunker = JapaneseTokenChunker(config)\n",
    "\n",
    "if os.environ.get(\"OPENAI_API_KEY\") == \"mock\":\n",
    "    embedder = MockEmbeddingService(config)\n",
    "else:\n",
    "    # Reuse imported EmbeddingService\n",
    "    embedder = EmbeddingService(config)\n",
    "\n",
    "# Summarization Agent (Mock mode handled internally by checking API key)\n",
    "summarizer = SummarizationAgent(config)\n",
    "\n",
    "logger.info(\"Components initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: Quickstart (Chunking)\n",
    "\n",
    "# Sample Text (Investment Philosophy style) - Extended to ensure clustering happens\n",
    "sample_text_base = \"\"\"\n",
    "é•·æœŸæŠ•è³‡ã®åŸºæœ¬ã¯ã€ä¼æ¥­ã®æˆé•·ã¨å…±ã«è³‡ç”£ã‚’å¢—ã‚„ã™ã“ã¨ã§ã™ã€‚\n",
    "çŸ­æœŸçš„ãªå¸‚å ´ã®å¤‰å‹•ã«æƒ‘ã‚ã•ã‚Œãšã€æœ¬è³ªçš„ãªä¾¡å€¤ã‚’è¦‹æ¥µã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "è¤‡åˆ©ã®åŠ¹æžœã¯æ™‚é–“ã‚’å‘³æ–¹ã«ã¤ã‘ã‚‹ã“ã¨ã§æœ€å¤§åŒ–ã•ã‚Œã¾ã™ã€‚\n",
    "é›ªã ã‚‹ã¾å¼ã«è³‡ç”£ãŒå¢—ãˆã‚‹ã“ã®ä»•çµ„ã¿ã‚’ç†è§£ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚\n",
    "\n",
    "ãƒªã‚¹ã‚¯ç®¡ç†ã¯åˆ†æ•£æŠ•è³‡ã«ã‚ˆã£ã¦è¡Œã„ã¾ã™ã€‚\n",
    "ä¸€ã¤ã®ã‚«ã‚´ã«ã™ã¹ã¦ã®åµã‚’ç››ã‚‹ãªã€ã¨ã„ã†æ ¼è¨€ã®é€šã‚Šã§ã™ã€‚\n",
    "\n",
    "æœ€å¾Œã«ã€è‡ªå·±ã¸ã®æŠ•è³‡ã‚‚å¿˜ã‚Œã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚\n",
    "çŸ¥è­˜ã“ããŒæœ€å¤§ã®é˜²å¾¡ã§ã‚ã‚Šã€æœ€å¤§ã®æ­¦å™¨ã¨ãªã‚‹ã®ã§ã™ã€‚\n",
    "\"\"\"\n",
    "\n",
    "# Repeat text to simulate a larger document for clustering demonstration\n",
    "sample_text = (sample_text_base + \"\\n\\n\") * 5\n",
    "\n",
    "logger.info(\"Starting Scenario 1: Chunking...\")\n",
    "\n",
    "# Chunk the text\n",
    "chunks_iter = chunker.split_text(sample_text, config)\n",
    "chunks = list(chunks_iter)\n",
    "\n",
    "logger.info(f\"Generated {len(chunks)} chunks.\")\n",
    "\n",
    "# Visualizing Chunks\n",
    "chunk_data = [{\"Index\": c.index, \"Text\": c.text[:50] + \"...\"} for c in chunks[:5]]\n",
    "\n",
    "table = mo.ui.table(chunk_data, label=\"First 5 Chunks\")\n",
    "\n",
    "logger.info(\"âœ… Scenario 1 Passed: Text chunked successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2: Embedding & Clustering Visualization\n",
    "\n",
    "logger.info(\"Starting Scenario 2: Embedding & Visualization...\")\n",
    "\n",
    "# Embed chunks (Mock or Real)\n",
    "# We consume the iterator to get a list\n",
    "embedded_chunks = list(embedder.embed_chunks(chunks))\n",
    "\n",
    "# Extract embeddings for visualization\n",
    "embeddings = [c.embedding for c in embedded_chunks if c.embedding]\n",
    "\n",
    "if not embeddings:\n",
    "    logger.warning(\"No embeddings generated.\")\n",
    "else:\n",
    "    # Simple 2D Visualization (using first 2 dims or random if high dim)\n",
    "    # Since MockEmbedder returns 384 dims, we just plot dim 0 vs dim 1\n",
    "    x = [e[0] for e in embeddings]\n",
    "    y = [e[1] for e in embeddings]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.scatter(x, y, alpha=0.7)\n",
    "    ax.set_title(\"Chunk Embeddings Projection (Dim 0 vs Dim 1)\")\n",
    "    ax.set_xlabel(\"Dimension 0\")\n",
    "    ax.set_ylabel(\"Dimension 1\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # In Marimo, we can display the figure\n",
    "    # plt.show() # Not needed in Marimo if we return the fig or use mo.mpl\n",
    "\n",
    "logger.info(\"âœ… Scenario 2 Passed: Embeddings generated and visualized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 3: Full RAPTOR Pipeline (Cycle 01)\n",
    "\n",
    "logger.info(\"Starting Scenario 3: Full RAPTOR Pipeline...\")\n",
    "\n",
    "clusterer = GMMClusterer()\n",
    "\n",
    "# Initialize Raptor Engine\n",
    "engine = RaptorEngine(\n",
    "    chunker=chunker,\n",
    "    embedder=embedder,\n",
    "    clusterer=clusterer,\n",
    "    summarizer=summarizer,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Run Engine\n",
    "tree = engine.run(sample_text, store=store)\n",
    "\n",
    "logger.info(f\"Tree generation complete. Root ID: {tree.root_node.id}\")\n",
    "\n",
    "# Validation\n",
    "root_node = tree.root_node\n",
    "logger.info(f\"Root Node DIKW Level: {root_node.metadata.dikw_level}\")\n",
    "\n",
    "assert root_node is not None\n",
    "assert root_node.metadata.dikw_level.value in [\"wisdom\", \"knowledge\", \"information\"]\n",
    "\n",
    "logger.info(\"âœ… Scenario 3 Passed: RAPTOR Engine executed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 4: Export & Visualization (Cycle 04/05)\n",
    "\n",
    "logger.info(\"Starting Scenario 4: Export & Visualization...\")\n",
    "\n",
    "# 1. Export to Markdown\n",
    "markdown_output = export_to_markdown(tree, store)\n",
    "\n",
    "# Save to file (optional, but good for verification)\n",
    "with open(\"tutorials/summary_all.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(markdown_output)\n",
    "\n",
    "# 2. Export to Canvas\n",
    "exporter = ObsidianCanvasExporter()\n",
    "exporter.export(tree, Path(\"tutorials/summary_kj.canvas\"), store)\n",
    "\n",
    "logger.info(\"âœ… Scenario 4 Passed: Exported to Markdown and Canvas.\")\n",
    "\n",
    "# Display Markdown Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 5: Interactive Refinement (Cycle 02/04)\n",
    "\n",
    "logger.info(\"Starting Scenario 5: Interactive Refinement...\")\n",
    "\n",
    "interactive_engine = InteractiveRaptorEngine(\n",
    "    store=store,\n",
    "    summarizer=summarizer,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# 1. Select a node to refine (Root node for simplicity)\n",
    "node_id = root_node.id\n",
    "instruction = \"Make it more concise and focused on risk.\"\n",
    "\n",
    "logger.info(f\"Refining node {node_id} with instruction: '{instruction}'\")\n",
    "\n",
    "# 2. Call Refine\n",
    "updated_node = interactive_engine.refine_node(node_id, instruction)\n",
    "\n",
    "# 3. Validation\n",
    "assert updated_node.id == node_id\n",
    "assert updated_node.metadata.is_user_edited is True\n",
    "assert instruction in updated_node.metadata.refinement_history\n",
    "\n",
    "print(f\"Original Text: {root_node.text[:50]}...\")\n",
    "print(f\"Refined Text:  {updated_node.text[:50]}...\")\n",
    "\n",
    "# Check if text changed\n",
    "assert updated_node.text != root_node.text or \"Summary of\" in updated_node.text\n",
    "\n",
    "logger.info(\"âœ… Scenario 5 Passed: Node refinement verified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 6: Traceability (Cycle 05)\n",
    "\n",
    "logger.info(\"Starting Scenario 6: Traceability...\")\n",
    "\n",
    "# 1. Get Source Chunks for the node\n",
    "source_chunks = list(interactive_engine.get_source_chunks(node_id))\n",
    "\n",
    "logger.info(f\"Found {len(source_chunks)} source chunks.\")\n",
    "\n",
    "# 2. Validation\n",
    "assert len(source_chunks) > 0\n",
    "\n",
    "first_chunk_text = source_chunks[0].text\n",
    "assert len(first_chunk_text) > 0\n",
    "\n",
    "# Ensure the chunk is actually part of the original text\n",
    "# We clean whitespace for comparison as chunking might affect spacing slightly\n",
    "clean_chunk = first_chunk_text.replace(\"\\n\", \"\").replace(\" \", \"\")\n",
    "clean_sample = sample_text.replace(\"\\n\", \"\").replace(\" \", \"\")\n",
    "\n",
    "assert clean_chunk in clean_sample\n",
    "\n",
    "print(f\"Source Chunk 1: {first_chunk_text[:50]}...\")\n",
    "\n",
    "logger.info(\"âœ… Scenario 6 Passed: Source chunks retrieved and verified against original text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final: Launching the GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "logger.info(\"ðŸŽ‰ All Systems Go: Matome 2.0 is ready for Knowledge Installation.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "marimo": {
   "app_config": {
    "width": "medium"
   },
   "marimo_version": "0.19.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
