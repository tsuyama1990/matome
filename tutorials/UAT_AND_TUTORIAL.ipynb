{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from typing import Iterator, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import marimo as mo\n",
    "\n",
    "# Adjust path to include src if running from root or tutorials\n",
    "current_dir = Path.cwd()\n",
    "if (current_dir / \"src\").exists():\n",
    "    sys.path.append(str(current_dir / \"src\"))\n",
    "elif (current_dir.parent / \"src\").exists():\n",
    "    sys.path.append(str(current_dir.parent / \"src\"))\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(\"matome.uat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJUe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo.md(\n",
    "    \"\"\"\n",
    "    # Matome 2.0: User Acceptance Test & Tutorial\n",
    "\n",
    "    This notebook demonstrates the core capabilities of the Matome 2.0 \"Knowledge Installation\" system.\n",
    "    It covers the entire pipeline from raw text to a structured, interactive knowledge base.\n",
    "\n",
    "    **Scenarios:**\n",
    "    1.  **Quickstart**: Text Ingestion & Chunking (The Basics)\n",
    "    2.  **Clustering**: Semantic Embedding & Grouping (The Engine)\n",
    "    3.  **Raptor Pipeline**: Recursive Summarization (The \"Aha!\" Moment)\n",
    "    4.  **Visualization**: Obsidian Canvas Export (The Output)\n",
    "\n",
    "    **Modes:**\n",
    "    *   **Real Mode**: Uses OpenAI/OpenRouter API for actual summarization (Requires `OPENROUTER_API_KEY`).\n",
    "    *   **Mock Mode**: Uses random embeddings and dummy summaries (Default if no key found).\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Determine Mode\n",
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "mock_mode = not bool(api_key)\n",
    "\n",
    "if mock_mode:\n",
    "    mode_msg = \"⚠️ **MOCK MODE ACTIVE** (No API Key found). Using dummy data.\"\n",
    "else:\n",
    "    mode_msg = \"✅ **REAL MODE ACTIVE**. Using live API.\"\n",
    "\n",
    "mo.md(mode_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration & Mocks ---\n",
    "from domain_models.config import ProcessingConfig\n",
    "from domain_models.manifest import Chunk\n",
    "from matome.engines.embedder import EmbeddingService\n",
    "from matome.agents.summarizer import SummarizationAgent\n",
    "from matome.interfaces import PromptStrategy\n",
    "\n",
    "# Initialize Config\n",
    "config = ProcessingConfig()\n",
    "\n",
    "# Mock Classes\n",
    "class MockEmbeddingService(EmbeddingService):\n",
    "    \"\"\"Generates random embeddings for testing.\"\"\"\n",
    "    def __init__(self, config: ProcessingConfig):\n",
    "        super().__init__(config)\n",
    "        self.dim = 384  # Simulating all-MiniLM-L6-v2\n",
    "\n",
    "    def embed_strings(self, texts: list[str] | tuple[str, ...]) -> Iterator[list[float]]:\n",
    "        for _ in texts:\n",
    "            # Deterministic random for stability\n",
    "            yield list(np.random.rand(self.dim))\n",
    "\n",
    "    def embed_chunks(self, chunks: list[Chunk]) -> Iterator[Chunk]:\n",
    "        for chunk in chunks:\n",
    "            chunk.embedding = list(np.random.rand(self.dim))\n",
    "            yield chunk\n",
    "\n",
    "class MockSummarizationAgent(SummarizationAgent):\n",
    "    \"\"\"Generates dummy summaries.\"\"\"\n",
    "    def __init__(self, config: ProcessingConfig):\n",
    "        self.config = config\n",
    "        self.mock_mode = True\n",
    "        self.model_name = \"mock-model\"\n",
    "        self.llm = None\n",
    "\n",
    "    def summarize(\n",
    "        self,\n",
    "        text: str,\n",
    "        config: ProcessingConfig | None = None,\n",
    "        strategy: PromptStrategy | None = None,\n",
    "        context: dict | None = None,\n",
    "    ) -> str:\n",
    "        prefix = \"Summary\"\n",
    "        if strategy:\n",
    "            # strategy.dikw_level is an Enum\n",
    "            try:\n",
    "                level = strategy.dikw_level.value\n",
    "            except AttributeError:\n",
    "                level = \"UNKNOWN\"\n",
    "            prefix = f\"[{str(level).upper()}] Summary\"\n",
    "        return f\"{prefix} of: {text[:30]}... (Mocked Content)\"\n",
    "\n",
    "# Factory\n",
    "def get_services(cfg, is_mock):\n",
    "    if is_mock:\n",
    "        return MockEmbeddingService(cfg), MockSummarizationAgent(cfg)\n",
    "    else:\n",
    "        return EmbeddingService(cfg), SummarizationAgent(cfg)\n",
    "\n",
    "mo.md(\"### System Configuration Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 0: Setup Test Data ---\n",
    "test_data_dir = Path(\"test_data\")\n",
    "test_data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "sample_file = test_data_dir / \"sample.txt\"\n",
    "if not sample_file.exists():\n",
    "    sample_file.write_text(\n",
    "        \"Matome 2.0 is a system for knowledge installation. \"\n",
    "        \"It uses recursive summarization to build a tree of knowledge. \"\n",
    "        \"This allows users to zoom from high-level wisdom to low-level data. \"\n",
    "        \"The system is built on Python and uses modern libraries like Pydantic and LangChain. \"\n",
    "        \"It supports both batch processing and interactive refinement. \" * 5,\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "target_file = test_data_dir / \"エミン流「会社四季報」最強の読み方.txt\"\n",
    "if not target_file.exists():\n",
    "    target_file.write_text(\n",
    "        \"会社四季報は、日本の全上場企業のデータが網羅された書籍です。\\n\"\n",
    "        \"エミン・ユルマズ氏は、四季報を「お宝の山」と呼びます。\\n\"\n",
    "        \"彼の読み方は、単なる数字の確認にとどまりません。\\n\"\n",
    "        \"業績の変化、株主構成、そして企業のコメント欄を入念にチェックします。\\n\"\n",
    "        \"特に「ニコちゃんマーク」の変化に注目することで、成長株を早期に発見できるのです。\\n\" * 20,\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "mo.md(f\"### Test Data Ready\\n- `{sample_file}`\\n- `{target_file}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Quickstart (Chunking) ---\n",
    "from matome.engines.token_chunker import JapaneseTokenChunker\n",
    "\n",
    "mo.md(\"## 1. Quickstart: Text Chunking\")\n",
    "\n",
    "# Load text\n",
    "quickstart_text = sample_file.read_text(encoding=\"utf-8\")\n",
    "\n",
    "# Initialize Chunker\n",
    "quickstart_chunker = JapaneseTokenChunker(config)\n",
    "\n",
    "# Execute Chunking\n",
    "quickstart_chunks = list(quickstart_chunker.split_text(quickstart_text, config))\n",
    "\n",
    "# Visualize\n",
    "chunk_preview = \"\\n\".join([f\"- **Chunk {c.index}**: {c.text[:50]}...\" for c in quickstart_chunks[:5]])\n",
    "\n",
    "mo.md(f\"### Chunking Results\\nGenerated **{len(quickstart_chunks)}** chunks.\\n\\n{chunk_preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Clustering Deep Dive ---\n",
    "from matome.engines.cluster import GMMClusterer\n",
    "\n",
    "mo.md(\"## 2. Clustering Deep Dive\")\n",
    "\n",
    "# Setup\n",
    "clustering_embedder, _ = get_services(config, mock_mode)\n",
    "clustering_chunker = JapaneseTokenChunker(config)\n",
    "clustering_clusterer = GMMClusterer()\n",
    "\n",
    "# Process\n",
    "text_cluster = sample_file.read_text(encoding=\"utf-8\")\n",
    "chunks_cluster = list(clustering_chunker.split_text(text_cluster, config))\n",
    "\n",
    "# Embed & Cluster\n",
    "embedded_chunks_iter = clustering_embedder.embed_chunks(chunks_cluster)\n",
    "\n",
    "# Manually collect for the clusterer input\n",
    "embeddings_input = []\n",
    "for chunk in embedded_chunks_iter:\n",
    "    if chunk.embedding:\n",
    "        embeddings_input.append((str(chunk.index), chunk.embedding))\n",
    "\n",
    "# Run Clustering\n",
    "# Create a config suitable for small data (force fewer clusters)\n",
    "cluster_config = ProcessingConfig(n_clusters=2, write_batch_size=10)\n",
    "\n",
    "clustering_clusters = clustering_clusterer.cluster_nodes(embeddings_input, cluster_config)\n",
    "\n",
    "cluster_info = \"\\n\".join([f\"- **Cluster {c.id}**: {len(c.node_indices)} nodes\" for c in clustering_clusters])\n",
    "\n",
    "mo.md(f\"### Clustering Results\\nGenerated **{len(clustering_clusters)}** clusters.\\n\\n{cluster_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Full Raptor Pipeline ---\n",
    "from matome.engines.raptor import RaptorEngine\n",
    "\n",
    "mo.md(\"## 3. Full Raptor Pipeline (The 'Aha!' Moment)\")\n",
    "\n",
    "# Setup Components\n",
    "chunker_raptor = JapaneseTokenChunker(config)\n",
    "clusterer_raptor = GMMClusterer()\n",
    "embedder_raptor, summarizer_raptor = get_services(config, mock_mode)\n",
    "\n",
    "raptor_engine = RaptorEngine(\n",
    "    chunker=chunker_raptor,\n",
    "    embedder=embedder_raptor,\n",
    "    clusterer=clusterer_raptor,\n",
    "    summarizer=summarizer_raptor,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Run Pipeline\n",
    "raptor_target_text = target_file.read_text(encoding=\"utf-8\")\n",
    "# Using default ephemeral store logic here for demonstration, \n",
    "# but typically one would provide a store. \n",
    "# We will do a full run with store in the next step/cell to enable visualization.\n",
    "\n",
    "# For now, let's run it to verify the pipeline works in memory/temp.\n",
    "raptor_tree = raptor_engine.run(raptor_target_text)\n",
    "\n",
    "summary_content = f\"# Summary Tree\\n\\nRoot: {raptor_tree.root_node.text}\\n\"\n",
    "\n",
    "output_md = Path(\"summary_all.md\")\n",
    "output_md.write_text(summary_content, encoding=\"utf-8\")\n",
    "\n",
    "mo.md(\n",
    "    f\"### Pipeline Complete\\n\"\n",
    "    f\"Generated Tree with Root Level: **{raptor_tree.root_node.level}**\\n\"\n",
    "    f\"Output saved to `{output_md}`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Visualization (Obsidian Canvas) ---\n",
    "from matome.exporters.obsidian import ObsidianCanvasExporter\n",
    "from matome.utils.store import DiskChunkStore\n",
    "\n",
    "mo.md(\"## 4. Visualization (Obsidian Canvas)\")\n",
    "\n",
    "# Re-run for export with persistent store\n",
    "store_path = Path(\"tutorials/chunks.db\")\n",
    "if store_path.exists():\n",
    "    store_path.unlink() # Clean start\n",
    "\n",
    "store = DiskChunkStore(db_path=store_path)\n",
    "\n",
    "# Setup again\n",
    "chunker_r = JapaneseTokenChunker(config)\n",
    "clusterer_r = GMMClusterer()\n",
    "embedder_r, summarizer_r = get_services(config, mock_mode)\n",
    "\n",
    "engine_r = RaptorEngine(\n",
    "    chunker=chunker_r,\n",
    "    embedder=embedder_r,\n",
    "    clusterer=clusterer_r,\n",
    "    summarizer=summarizer_r,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Run with store\n",
    "tree_r = engine_r.run(raptor_target_text, store=store)\n",
    "\n",
    "# Export Canvas\n",
    "exporter = ObsidianCanvasExporter(config)\n",
    "output_canvas = Path(\"summary_kj.canvas\")\n",
    "exporter.export(tree_r, output_canvas, store)\n",
    "\n",
    "mo.md(\n",
    "    f\"### Canvas Exported\\n\"\n",
    "    f\"File saved to: `{output_canvas}`\\n\"\n",
    "    f\"Database: `{store_path}`\\n\\n\"\n",
    "    f\"**Next Step**: Open `{output_canvas}` in Obsidian to visualize!\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "marimo": {
   "app_config": {
    "width": "medium"
   },
   "marimo_version": "0.19.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
