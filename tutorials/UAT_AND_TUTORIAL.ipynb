{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Iterator, Iterable, Any\n",
    "import numpy as np\n",
    "import marimo as mo\n",
    "\n",
    "# Adjust path to include src if running from root or tutorials\n",
    "current_dir = Path.cwd()\n",
    "if (current_dir / \"src\").exists():\n",
    "    sys.path.append(str(current_dir / \"src\"))\n",
    "elif (current_dir.parent / \"src\").exists():\n",
    "    sys.path.append(str(current_dir.parent / \"src\"))\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(\"matome.uat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJUe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo.md(\n",
    "    \"\"\"\n",
    "    # Matome 2.0: User Acceptance Test & Tutorial\n",
    "\n",
    "    This notebook demonstrates the core capabilities of the Matome 2.0 \"Knowledge Installation\" system.\n",
    "    It covers the entire pipeline from raw text to a structured, interactive knowledge base.\n",
    "\n",
    "    **Scenarios:**\n",
    "    1.  **Cycle 01: DIKW Generation**: Generate a tree where Root is Wisdom.\n",
    "    2.  **Cycle 03: Semantic Zooming**: Traverse from Wisdom -> Knowledge -> Information -> Data.\n",
    "    3.  **Cycle 02/04: Interactive Refinement**: Refine a node and verify persistence.\n",
    "    4.  **Cycle 05: Traceability**: Verify source chunks for a summary node.\n",
    "    5.  **GUI Launch**: Instructions to explore visually.\n",
    "\n",
    "    **Modes:**\n",
    "    *   **Real Mode**: Uses OpenAI/OpenRouter API for actual summarization (Requires `OPENROUTER_API_KEY`).\n",
    "    *   **Mock Mode**: Uses random embeddings and dummy summaries (Default if no key found).\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Determine Mode\n",
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "mock_mode = not bool(api_key) or api_key == \"mock\"\n",
    "\n",
    "if mock_mode:\n",
    "    mode_msg = \"âš ï¸ **MOCK MODE ACTIVE** (No API Key found or set to 'mock'). Using dummy data.\"\n",
    "else:\n",
    "    mode_msg = \"âœ… **REAL MODE ACTIVE**. Using live API.\"\n",
    "\n",
    "mo.md(mode_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration & Mocks ---\n",
    "from domain_models.config import ProcessingConfig\n",
    "from domain_models.manifest import Chunk, SummaryNode\n",
    "from domain_models.types import DIKWLevel, NodeID\n",
    "from matome.engines.embedder import EmbeddingService\n",
    "from matome.agents.summarizer import SummarizationAgent\n",
    "from matome.interfaces import PromptStrategy\n",
    "\n",
    "# Initialize Config\n",
    "# Ensure strict consistency for testing\n",
    "config = ProcessingConfig(\n",
    "    max_tokens=1000,\n",
    "    max_summary_tokens=200, # Keep summaries concise\n",
    "    dikw_topology={\n",
    "        \"root\": DIKWLevel.WISDOM,\n",
    "        \"intermediate\": DIKWLevel.KNOWLEDGE,\n",
    "        \"leaf\": DIKWLevel.INFORMATION,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Mock Classes\n",
    "class MockEmbeddingService(EmbeddingService):\n",
    "    \"\"\"Generates random embeddings for testing.\"\"\"\n",
    "    def __init__(self, config: ProcessingConfig):\n",
    "        super().__init__(config)\n",
    "        self.dim = 384  # Simulating all-MiniLM-L6-v2\n",
    "\n",
    "    def embed_strings(self, texts: list[str] | tuple[str, ...]) -> Iterator[list[float]]:\n",
    "        for _ in texts:\n",
    "            # Deterministic random for stability based on length\n",
    "            yield list(np.random.rand(self.dim))\n",
    "\n",
    "    def embed_chunks(self, chunks: list[Chunk]) -> Iterator[Chunk]:\n",
    "        for chunk in chunks:\n",
    "            chunk.embedding = list(np.random.rand(self.dim))\n",
    "            yield chunk\n",
    "\n",
    "class MockSummarizationAgent(SummarizationAgent):\n",
    "    \"\"\"Generates dummy summaries respecting DIKW levels.\"\"\"\n",
    "    def __init__(self, config: ProcessingConfig):\n",
    "        self.config = config\n",
    "        self.mock_mode = True\n",
    "        self.model_name = \"mock-model\"\n",
    "        self.llm = None\n",
    "\n",
    "    def summarize(\n",
    "        self,\n",
    "        text: str,\n",
    "        config: ProcessingConfig | None = None,\n",
    "        strategy: PromptStrategy | None = None,\n",
    "        context: dict[str, Any] | None = None,\n",
    "    ) -> str:\n",
    "        prefix = \"Summary\"\n",
    "        if strategy:\n",
    "            # strategy.dikw_level is likely a DIKWLevel Enum member\n",
    "            try:\n",
    "                # Access the .value if it's an enum, or just str()\n",
    "                level = getattr(strategy, \"target_dikw_level\", \"UNKNOWN\")\n",
    "                # If it's an Enum, get value\n",
    "                if hasattr(level, \"value\"):\n",
    "                    level = level.value\n",
    "            except AttributeError:\n",
    "                level = \"UNKNOWN\"\n",
    "\n",
    "            # Check context for instruction (Refinement)\n",
    "            instruction = context.get(\"instruction\") if context else None\n",
    "            if instruction:\n",
    "                return f\"[REFINED] {instruction} -> {text[:30]}...\"\n",
    "\n",
    "            prefix = f\"[{str(level).upper()}] Summary\"\n",
    "\n",
    "        return f\"{prefix} of: {text[:30]}... (Mocked Content)\"\n",
    "\n",
    "# Factory\n",
    "def get_services(cfg, is_mock):\n",
    "    if is_mock:\n",
    "        return MockEmbeddingService(cfg), MockSummarizationAgent(cfg)\n",
    "    else:\n",
    "        return EmbeddingService(cfg), SummarizationAgent(cfg)\n",
    "\n",
    "mo.md(\"### System Configuration Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 0: Setup Test Data ---\n",
    "test_data_dir = Path(\"test_data\")\n",
    "test_data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "sample_file = test_data_dir / \"sample.txt\"\n",
    "if not sample_file.exists():\n",
    "    sample_file.write_text(\n",
    "        \"Matome 2.0 is a system for knowledge installation. \"\n",
    "        \"It uses recursive summarization to build a tree of knowledge. \"\n",
    "        \"This allows users to zoom from high-level wisdom to low-level data. \"\n",
    "        \"The system is built on Python and uses modern libraries like Pydantic and LangChain. \"\n",
    "        \"It supports both batch processing and interactive refinement. \" * 10,\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "mo.md(f\"### Test Data Ready\\n- `{sample_file}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Cycle 01 - DIKW Generation ---\n",
    "from matome.engines.raptor import RaptorEngine\n",
    "from matome.engines.token_chunker import JapaneseTokenChunker\n",
    "from matome.engines.cluster import GMMClusterer\n",
    "from matome.utils.store import DiskChunkStore\n",
    "\n",
    "mo.md(\"## 1. Cycle 01: DIKW Generation\")\n",
    "\n",
    "# Clean DB\n",
    "store_path = Path(\"tutorials/chunks.db\")\n",
    "if store_path.exists():\n",
    "    store_path.unlink()\n",
    "\n",
    "store = DiskChunkStore(db_path=store_path)\n",
    "\n",
    "# Setup Components\n",
    "chunker = JapaneseTokenChunker(config)\n",
    "clusterer = GMMClusterer()\n",
    "embedder, summarizer = get_services(config, mock_mode)\n",
    "\n",
    "engine = RaptorEngine(\n",
    "    chunker=chunker,\n",
    "    embedder=embedder,\n",
    "    clusterer=clusterer,\n",
    "    summarizer=summarizer,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Run Pipeline\n",
    "text = sample_file.read_text(encoding=\"utf-8\")\n",
    "tree = engine.run(text, store=store)\n",
    "\n",
    "# Verify Root is Wisdom\n",
    "root_node = tree.root_node\n",
    "# Note: In mock mode, if single chunk, it might default to WisdomStrategy manually in code\n",
    "# or if recursion happened, it uses topology.\n",
    "\n",
    "# Check DIKW level in metadata\n",
    "dikw_level = root_node.metadata.dikw_level\n",
    "\n",
    "# Assertion\n",
    "assert dikw_level == DIKWLevel.WISDOM, f\"Root node should be WISDOM, got {dikw_level}\"\n",
    "\n",
    "mo.md(\n",
    "    f\"### DIKW Generation Successful\\n\"\n",
    "    f\"Root Node ID: `{root_node.id}`\\n\"\n",
    "    f\"DIKW Level: **{dikw_level}**\\n\"\n",
    "    f\"Text Preview: {root_node.text[:100]}...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Cycle 03 - Semantic Zooming ---\n",
    "mo.md(\"## 2. Cycle 03: Semantic Zooming\")\n",
    "\n",
    "# Traverse hierarchy\n",
    "# Root (Wisdom) -> Children (Knowledge) -> Children (Information) -> Data (Chunks)\n",
    "\n",
    "layers = {}\n",
    "\n",
    "# Layer 1: Wisdom (Root)\n",
    "layers[1] = [tree.root_node]\n",
    "\n",
    "# Get children recursively\n",
    "def get_children_nodes(parent_nodes):\n",
    "    children = []\n",
    "    for p in parent_nodes:\n",
    "        # child indices can be str (SummaryNode) or int (Chunk)\n",
    "        child_ids = [str(c) for c in p.children_indices]\n",
    "        nodes = list(store.get_nodes(child_ids))\n",
    "        children.extend([n for n in nodes if n is not None])\n",
    "    return children\n",
    "\n",
    "# Layer 2: Knowledge (Intermediate)\n",
    "# Depending on text size, we might skip straight to Information if small.\n",
    "# But let's see what we got.\n",
    "\n",
    "current_layer_nodes = layers[1]\n",
    "depth = 1\n",
    "\n",
    "hierarchy_desc = []\n",
    "hierarchy_desc.append(f\"**Level {depth} ({current_layer_nodes[0].metadata.dikw_level})**: {len(current_layer_nodes)} node(s)\")\n",
    "\n",
    "while True:\n",
    "    next_nodes = get_children_nodes(current_layer_nodes)\n",
    "    if not next_nodes:\n",
    "        break\n",
    "\n",
    "    # Check if next nodes are chunks or summaries\n",
    "    first_child = next_nodes[0]\n",
    "    depth += 1\n",
    "\n",
    "    # SummaryNodes have 'children_indices', Chunks do not\n",
    "    if hasattr(first_child, \"children_indices\"): # SummaryNode\n",
    "         level_name = first_child.metadata.dikw_level\n",
    "         hierarchy_desc.append(f\"**Level {depth} ({level_name})**: {len(next_nodes)} node(s)\")\n",
    "         current_layer_nodes = next_nodes\n",
    "    else: # Chunk\n",
    "         hierarchy_desc.append(f\"**Level {depth} (DATA)**: {len(next_nodes)} chunk(s)\")\n",
    "         break\n",
    "\n",
    "mo.md(f\"### Hierarchy Verified\\n\" + \"\\n\".join([f\"- {h}\" for h in hierarchy_desc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Cycle 02/04 - Interactive Refinement ---\n",
    "from matome.engines.interactive_raptor import InteractiveRaptorEngine\n",
    "\n",
    "mo.md(\"## 3. Cycle 02/04: Interactive Refinement\")\n",
    "\n",
    "interactive_engine = InteractiveRaptorEngine(store, summarizer, config)\n",
    "\n",
    "# Pick the root node to refine\n",
    "target_node_id = root_node.id\n",
    "instruction = \"Explain like I'm 5\"\n",
    "\n",
    "# Refine\n",
    "refined_node = interactive_engine.refine_node(target_node_id, instruction)\n",
    "\n",
    "# Verify\n",
    "assert refined_node.metadata.is_user_edited == True\n",
    "assert instruction in refined_node.metadata.refinement_history\n",
    "\n",
    "# Verify persistence\n",
    "persisted_node = store.get_node(target_node_id)\n",
    "assert persisted_node.text == refined_node.text\n",
    "assert persisted_node.metadata.is_user_edited == True\n",
    "\n",
    "mo.md(\n",
    "    f\"### Refinement Successful\\n\"\n",
    "    f\"Node `{target_node_id}` updated.\\n\"\n",
    "    f\"Instruction: *'{instruction}'*\\n\"\n",
    "    f\"New Text: {refined_node.text[:100]}...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Cycle 05 - Traceability ---\n",
    "mo.md(\"## 4. Cycle 05: Traceability\")\n",
    "\n",
    "# Get source chunks for the root node\n",
    "source_chunks = list(interactive_engine.get_source_chunks(root_node.id))\n",
    "\n",
    "assert len(source_chunks) > 0\n",
    "first_chunk = source_chunks[0]\n",
    "\n",
    "mo.md(\n",
    "    f\"### Traceability Verified\\n\"\n",
    "    f\"Node `{root_node.id}` traces back to **{len(source_chunks)}** original chunks.\\n\"\n",
    "    f\"First Chunk Preview: *{first_chunk.text[:50]}...*\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5: GUI Launch ---\n",
    "mo.md(\n",
    "    f\"\"\"\n",
    "    ## ðŸŽ‰ All Systems Go!\n",
    "\n",
    "    The Matome 2.0 pipeline has been verified.\n",
    "    You can now launch the interactive GUI to explore the generated knowledge base.\n",
    "\n",
    "    Run this command in your terminal:\n",
    "    ```bash\n",
    "    uv run matome serve {store_path}\n",
    "    ```\n",
    "    \"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "marimo": {
   "app_config": {
    "width": "medium"
   },
   "marimo_version": "0.19.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
