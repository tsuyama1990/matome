{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import uuid\n",
    "from collections.abc import Iterable, Iterator\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Add src to path if running from root\n",
    "if str(Path.cwd()) not in sys.path:\n",
    "    sys.path.append(str(Path.cwd()))\n",
    "\n",
    "# Matome Imports\n",
    "from domain_models.config import ProcessingConfig, ClusteringAlgorithm\n",
    "from domain_models.manifest import Chunk, SummaryNode\n",
    "from matome.engines.token_chunker import JapaneseTokenChunker\n",
    "from matome.engines.raptor import RaptorEngine\n",
    "from matome.utils.store import DiskChunkStore\n",
    "from matome.agents.summarizer import SummarizationAgent\n",
    "from matome.engines.cluster import GMMClusterer\n",
    "from matome.exporters.obsidian import ObsidianCanvasExporter\n",
    "from matome.engines.embedder import EmbeddingService\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(\"matome.tutorial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJUe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockEmbeddingService(EmbeddingService):\n",
    "    \"\"\"Mock Embedding Service that returns random vectors.\"\"\"\n",
    "\n",
    "    def __init__(self, config: ProcessingConfig) -> None:\n",
    "        super().__init__(config)\n",
    "        self.dim = 384  # Standard small model dimension\n",
    "\n",
    "    def embed_strings(self, texts: Iterable[str]) -> Iterator[list[float]]:\n",
    "        \"\"\"Generate random embeddings for strings.\"\"\"\n",
    "        texts_list = list(texts)\n",
    "        count = len(texts_list)\n",
    "        logger.info(f\"[Mock] Generating {count} random embeddings (dim={self.dim})\")\n",
    "\n",
    "        # Generate deterministic random vectors based on text length to avoid flux\n",
    "        rng = np.random.default_rng(42)\n",
    "\n",
    "        for _ in range(count):\n",
    "            # Normalize to unit length like real embeddings\n",
    "            vec = rng.standard_normal(self.dim)\n",
    "            vec /= np.linalg.norm(vec)\n",
    "            yield vec.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration & Mock Setup ---\n",
    "\n",
    "# Check for API Key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\") or os.getenv(\"OPENROUTER_API_KEY\")\n",
    "is_mock_mode = not api_key or api_key == \"mock\" or api_key == \"\"\n",
    "\n",
    "if is_mock_mode:\n",
    "    print(\"‚ö†Ô∏è  No API Key found. Running in **MOCK MODE**.\")\n",
    "    os.environ[\"OPENROUTER_API_KEY\"] = \"mock\"\n",
    "    # Use MockEmbeddingService\n",
    "    EmbeddingServiceClass = MockEmbeddingService\n",
    "else:\n",
    "    print(\"‚úÖ API Key found. Running in **REAL MODE**.\")\n",
    "    from matome.engines.embedder import EmbeddingService as RealEmbeddingService\n",
    "    EmbeddingServiceClass = RealEmbeddingService\n",
    "\n",
    "# Setup Config\n",
    "config = ProcessingConfig(\n",
    "    summarization_model=\"openai/gpt-4o-mini\",\n",
    "    clustering_algorithm=\"gmm\",\n",
    "    max_tokens=100,  # Small chunks for tutorial\n",
    "    max_summary_tokens=100,  # Ensure summary is not larger than chunk\n",
    "    n_clusters=3, # Force small number of clusters\n",
    "    write_batch_size=10,\n",
    "    embedding_batch_size=10,\n",
    ")\n",
    "\n",
    "# Setup Paths\n",
    "TUTORIAL_DIR = Path(\"tutorials\")\n",
    "TUTORIAL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "DB_PATH = TUTORIAL_DIR / \"chunks.db\"\n",
    "if DB_PATH.exists():\n",
    "    try:\n",
    "        DB_PATH.unlink()\n",
    "        print(f\"üßπ Cleaned up existing database: {DB_PATH}\")\n",
    "    except PermissionError:\n",
    "        print(f\"‚ö†Ô∏è  Could not delete {DB_PATH}. Is it in use?\")\n",
    "\n",
    "CANVAS_PATH = TUTORIAL_DIR / \"summary_kj.canvas\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 1: Ingestion & Chunking ---\n",
    "\n",
    "# Sample Text (Excerpt about \"Wisdom\" vs \"Data\")\n",
    "SAMPLE_TEXT = \"\"\"\n",
    "In the modern age, we are drowning in data but starving for wisdom.\n",
    "Data is raw facts, unorganized and unprocessed.\n",
    "Information is data that is processed to be useful.\n",
    "Knowledge is the application of data and information.\n",
    "Wisdom is the ability to think and act using knowledge, experience, understanding, common sense and insight.\n",
    "\n",
    "The Matome system is designed to traverse this hierarchy.\n",
    "It starts by breaking down text into small, manageable chunks.\n",
    "These chunks are then clustered based on semantic similarity.\n",
    "Each cluster is summarized to create a higher-level node.\n",
    "This process repeats recursively until a single root node - Wisdom - is formed.\n",
    "\"\"\" * 5  # Repeat to ensure we have enough text for multiple chunks\n",
    "\n",
    "print(f\"üìÑ Loaded Sample Text ({len(SAMPLE_TEXT)} chars)\")\n",
    "\n",
    "# Initialize Chunker\n",
    "chunker = JapaneseTokenChunker(config)\n",
    "\n",
    "# Chunk the text\n",
    "print(\"‚úÇÔ∏è  Chunking text...\")\n",
    "initial_chunks = list(chunker.split_text(SAMPLE_TEXT, config))\n",
    "\n",
    "print(f\"‚úÖ Generated {len(initial_chunks)} chunks.\")\n",
    "for i, chunk in enumerate(initial_chunks[:3]):\n",
    "    print(f\"   - Chunk {i}: {chunk.text[:50]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 2 & 3: Raptor Pipeline (Clustering & Summarization) ---\n",
    "\n",
    "print(\"\\nüöÄ Initializing Raptor Engine...\")\n",
    "\n",
    "# Initialize Components\n",
    "embedder = EmbeddingServiceClass(config)\n",
    "clusterer = GMMClusterer() # No config in init\n",
    "summarizer = SummarizationAgent(config)\n",
    "chunker_instance = JapaneseTokenChunker(config)\n",
    "\n",
    "engine = RaptorEngine(\n",
    "    chunker=chunker_instance,\n",
    "    embedder=embedder,\n",
    "    clusterer=clusterer,\n",
    "    summarizer=summarizer,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Engine Initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üíæ Using Database: {DB_PATH}\")\n",
    "\n",
    "# Use context manager for store\n",
    "with DiskChunkStore(DB_PATH) as store:\n",
    "    print(\"‚ñ∂Ô∏è  Executing Pipeline (this may take a moment)...\")\n",
    "    # Run Raptor\n",
    "    # Note: RaptorEngine.run() uses its own store context if store is None, \n",
    "    # but here we pass an active store instance.\n",
    "    tree = engine.run(SAMPLE_TEXT, store=store)\n",
    "\n",
    "    print(\"\\n‚úÖ Pipeline Complete!\")\n",
    "\n",
    "    # Verify Root Node\n",
    "    root = tree.root_node\n",
    "    print(f\"üëë Root Node ID: {root.id}\")\n",
    "\n",
    "    level = getattr(root, \"level\", \"Unknown\")\n",
    "    dikw = \"Unknown\"\n",
    "    if hasattr(root, \"metadata\") and root.metadata:\n",
    "         dikw = root.metadata.dikw_level\n",
    "\n",
    "    print(f\"   Level: {level} ({dikw})\")\n",
    "    print(f\"   Summary: {root.text[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 4: Visualization & Export ---\n",
    "\n",
    "print(f\"\\nüé® Exporting to Canvas: {CANVAS_PATH}\")\n",
    "\n",
    "exporter = ObsidianCanvasExporter(config)\n",
    "\n",
    "# Re-open store to read nodes for export\n",
    "with DiskChunkStore(DB_PATH) as store_export:\n",
    "    exporter.export(tree, CANVAS_PATH, store_export)\n",
    "\n",
    "print(f\"‚úÖ Exported to {CANVAS_PATH}\")\n",
    "\n",
    "# Validation\n",
    "if CANVAS_PATH.exists():\n",
    "    print(\"üéâ Verification Successful: Canvas file created.\")\n",
    "else:\n",
    "    print(\"‚ùå Verification Failed: Canvas file not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Conclusion ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéâ All Systems Go: Matome 2.0 is ready for Knowledge Installation.\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nTo explore the results visually, run:\")\n",
    "print(f\"   uv run matome serve {DB_PATH}\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "marimo": {
   "app_config": {
    "width": "medium"
   },
   "marimo_version": "0.19.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
