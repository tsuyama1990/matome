{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Optional\n",
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Ensure src is in path\n",
    "project_root = Path.cwd()\n",
    "if str(project_root / \"src\") not in sys.path:\n",
    "    sys.path.append(str(project_root / \"src\"))\n",
    "\n",
    "# Matome Imports\n",
    "from domain_models.config import ProcessingConfig\n",
    "from domain_models.manifest import Chunk, SummaryNode, DocumentTree, Cluster\n",
    "from domain_models.types import DIKWLevel\n",
    "from matome.engines.raptor import RaptorEngine\n",
    "from matome.engines.interactive_raptor import InteractiveRaptorEngine\n",
    "from matome.engines.semantic_chunker import JapaneseSemanticChunker\n",
    "from matome.engines.embedder import EmbeddingService\n",
    "from matome.engines.cluster import GMMClusterer\n",
    "from matome.agents.summarizer import SummarizationAgent\n",
    "from matome.utils.store import DiskChunkStore\n",
    "from matome.utils.traversal import traverse_source_chunks\n",
    "from matome.exporters.markdown import export_to_markdown\n",
    "from matome.exporters.obsidian import ObsidianCanvasExporter\n",
    "\n",
    "# Configure Logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(\"UAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJUe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for API Key\n",
    "api_key = os.environ.get(\"OPENROUTER_API_KEY\")\n",
    "is_mock_mode = not api_key\n",
    "\n",
    "if is_mock_mode:\n",
    "    logger.info(\"‚ö†Ô∏è  OPENROUTER_API_KEY not found. Running in MOCK MODE.\")\n",
    "else:\n",
    "    logger.info(\"‚úÖ  OPENROUTER_API_KEY found. Running in REAL MODE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mocking Utilities\n",
    "\n",
    "def get_mock_embeddings(texts: list[str]):\n",
    "    # Return numpy array of shape (len(texts), 384)\n",
    "    return np.array([np.random.rand(384) for _ in texts])\n",
    "\n",
    "def mock_llm_call(*args, **kwargs):\n",
    "    return \"This is a mock summary generated by the UAT script.\"\n",
    "\n",
    "context_managers = []\n",
    "\n",
    "if is_mock_mode:\n",
    "    # Mock Embeddings\n",
    "    # We patch SentenceTransformer within EmbeddingService if possible, or higher level.\n",
    "\n",
    "    # Mock the SentenceTransformer class itself where it's imported in embedder\n",
    "    mock_model = MagicMock()\n",
    "    mock_model.encode.side_effect = lambda texts, **kwargs: get_mock_embeddings(texts)\n",
    "\n",
    "    p1 = patch(\"matome.engines.embedder.SentenceTransformer\", return_value=mock_model)\n",
    "\n",
    "    # Mock LLM\n",
    "    # Patch the SummarizationAgent.summarize method\n",
    "    p2 = patch(\"matome.agents.summarizer.SummarizationAgent.summarize\", return_value=\"Mock Summary Content\")\n",
    "\n",
    "    # Also need to mock ChatOpenAI if it's instantiated\n",
    "    p3 = patch(\"matome.agents.summarizer.ChatOpenAI\", new=MagicMock())\n",
    "\n",
    "    context_managers.extend([p1, p2, p3])\n",
    "    for _cm in context_managers:\n",
    "        _cm.start()\n",
    "\n",
    "    logger.info(\"Mocks activated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation Logic\n",
    "test_data_dir = project_root / \"test_data\"\n",
    "if not test_data_dir.exists():\n",
    "    os.makedirs(test_data_dir)\n",
    "    logger.info(f\"Created directory: {test_data_dir}\")\n",
    "\n",
    "# Create sample.txt (Short)\n",
    "sample_txt_path = test_data_dir / \"sample.txt\"\n",
    "if not sample_txt_path.exists():\n",
    "    dummy_text = \"„Åì„Çå„ÅØ„ÉÜ„Çπ„ÉàÁî®„ÅÆÊó•Êú¨Ë™û„ÉÜ„Ç≠„Çπ„Éà„Åß„Åô„ÄÇ\\n\" * 100\n",
    "    with open(sample_txt_path, \"w\", encoding=\"utf-8\") as _f_gen:\n",
    "        _f_gen.write(dummy_text)\n",
    "    logger.info(f\"Created dummy file: {sample_txt_path}\")\n",
    "\n",
    "# Create dummy long text (Emin Style)\n",
    "emin_txt_path = test_data_dir / \"„Ç®„Éü„É≥ÊµÅ„Äå‰ºöÁ§æÂõõÂ≠£Â†±„ÄçÊúÄÂº∑„ÅÆË™≠„ÅøÊñπ.txt\"\n",
    "if not emin_txt_path.exists():\n",
    "    # Generate ~10KB of dummy Japanese text with some structure\n",
    "    dummy_long_text = \"\"\n",
    "    keywords = [\"‰ºöÁ§æÂõõÂ≠£Â†±\", \"PSR\", \"ÊôÇ‰æ°Á∑èÈ°ç\", \"ÊàêÈï∑Ê†™\", \"„Éê„É™„É•„ÉºÊ†™\", \"„ÉÅ„É£„Éº„ÉàÂàÜÊûê\", \"Ê•≠Á∏æ‰øÆÊ≠£\", \"Â§ñÂõΩ‰∫∫ÊäïË≥áÂÆ∂\", \"Ê±∫ÁÆóÁô∫Ë°®\", \"Â¢óÁõä\"]\n",
    "    for _idx_gen in range(500):\n",
    "        sentence = f\"Á¨¨{_idx_gen}Á´†: {random.choice(keywords)}„Å´„Å§„ÅÑ„Å¶ËÄÉ„Åà„Çã„ÄÇÂ∏ÇÂ†¥„ÅÆÂãïÂêë„ÇíË¶ã„Çã‰∏ä„Åß{random.choice(keywords)}„ÅØÈáçË¶Å„Åß„ÅÇ„Çã„ÄÇ\\n\"\n",
    "        dummy_long_text += sentence\n",
    "\n",
    "    with open(emin_txt_path, \"w\", encoding=\"utf-8\") as _f_long:\n",
    "        _f_long.write(dummy_long_text)\n",
    "    logger.info(f\"Created dummy long text file: {emin_txt_path} ({len(dummy_long_text)} chars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: Quickstart (Data Loading)\n",
    "logger.info(\"--- Scenario 1: Quickstart ---\")\n",
    "\n",
    "# Load Text\n",
    "with open(sample_txt_path, \"r\", encoding=\"utf-8\") as _f_sc1:\n",
    "    content_sample = _f_sc1.read()\n",
    "\n",
    "logger.info(f\"Loaded {len(content_sample)} chars from {sample_txt_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1 Continued: Chunking Visualization\n",
    "\n",
    "# Initialize basic config for chunking\n",
    "config_chunking = ProcessingConfig(\n",
    "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    chunk_buffer_size=10,\n",
    ")\n",
    "\n",
    "# We need an embedder for semantic chunking\n",
    "embedder_chunking = EmbeddingService(config_chunking)\n",
    "chunker_simple = JapaneseSemanticChunker(embedder_chunking)\n",
    "\n",
    "logger.info(\"Splitting text into chunks...\")\n",
    "chunks_sample = list(chunker_simple.split_text(content_sample, config_chunking))\n",
    "\n",
    "logger.info(f\"Generated {len(chunks_sample)} chunks.\")\n",
    "\n",
    "print(\"--- First 5 Chunks ---\")\n",
    "for _idx_chunk, c in enumerate(chunks_sample[:5]):\n",
    "    print(f\"[{c.index}] {c.text[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2: Clustering Deep Dive (Visualization)\n",
    "logger.info(\"--- Scenario 2: Clustering Deep Dive ---\")\n",
    "\n",
    "# Generate embeddings for the chunks\n",
    "texts = [c.text for c in chunks_sample]\n",
    "embeddings = list(embedder_chunking.embed_strings(texts))\n",
    "embeddings_np = np.array(embeddings)\n",
    "\n",
    "logger.info(f\"Generated embeddings shape: {embeddings_np.shape}\")\n",
    "\n",
    "# Reduce dimensions to 2D\n",
    "n_samples = embeddings_np.shape[0]\n",
    "if n_samples < 2:\n",
    "    logger.warning(f\"Not enough samples for 2D PCA ({n_samples}). Skipping plot.\")\n",
    "    reduced_embeddings = np.zeros((n_samples, 2))\n",
    "    pca = None\n",
    "    plot_path = None\n",
    "else:\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings_np)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.7)\n",
    "    plt.title(\"Chunk Embeddings Visualization (PCA)\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plot_path = project_root / \"clustering_plot.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    logger.info(f\"Saved clustering plot to {plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 3: Full Raptor Pipeline\n",
    "logger.info(\"--- Scenario 3: Full Raptor Pipeline ---\")\n",
    "\n",
    "# Determine input file\n",
    "target_file = emin_txt_path\n",
    "\n",
    "with open(target_file, \"r\", encoding=\"utf-8\") as _f_sc3:\n",
    "    full_content = _f_sc3.read()\n",
    "\n",
    "# Initialize Full Engine\n",
    "# We use a persistent store for the tutorial so we can run the GUI later\n",
    "db_path = project_root / \"tutorials\" / \"chunks.db\"\n",
    "if db_path.exists():\n",
    "    try:\n",
    "        os.remove(db_path)\n",
    "        logger.info(f\"Removed existing DB: {db_path}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not remove existing DB: {e}\")\n",
    "\n",
    "store = DiskChunkStore(db_path=db_path)\n",
    "\n",
    "config = ProcessingConfig(\n",
    "    n_clusters=2,\n",
    "    umap_n_neighbors=2,\n",
    "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    summarization_model=\"openai/gpt-4o-mini\",\n",
    "    chunk_buffer_size=10,\n",
    "    max_summary_tokens=200, # Small summaries for speed\n",
    ")\n",
    "\n",
    "embedder = EmbeddingService(config)\n",
    "chunker = JapaneseSemanticChunker(embedder)\n",
    "clusterer = GMMClusterer()\n",
    "summarizer = SummarizationAgent(config)\n",
    "\n",
    "engine = RaptorEngine(\n",
    "    chunker=chunker,\n",
    "    embedder=embedder,\n",
    "    clusterer=clusterer,\n",
    "    summarizer=summarizer,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "logger.info(\"Running Raptor Engine...\")\n",
    "try:\n",
    "    tree = engine.run(full_content, store=store)\n",
    "    root_id = tree.root_node.id if isinstance(tree.root_node, SummaryNode) else str(tree.root_node.index)\n",
    "    logger.info(f\"Pipeline complete. Root Node ID: {root_id}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Pipeline failed: {e}\")\n",
    "    store.close()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 4: Validation (The \"Grok\" Moment)\n",
    "logger.info(\"--- Scenario 4: Validation (DIKW) ---\")\n",
    "\n",
    "root = tree.root_node\n",
    "if isinstance(root, SummaryNode):\n",
    "    # In mock/small data, it might not reach Wisdom level if depth is low\n",
    "    # But we check whatever level it reached.\n",
    "    logger.info(f\"Root Node Level: {root.level}\")\n",
    "    logger.info(f\"Root Node DIKW: {root.metadata.dikw_level}\")\n",
    "\n",
    "    # We ideally want Wisdom, but if text is short it might be Knowledge or Information\n",
    "    # Assert it's at least Information if we have summary nodes\n",
    "    assert root.metadata.dikw_level in [DIKWLevel.WISDOM, DIKWLevel.KNOWLEDGE, DIKWLevel.INFORMATION]\n",
    "    logger.info(\"‚úÖ Root Node Level Validated\")\n",
    "else:\n",
    "    logger.warning(\"Root is a Chunk (text too short for summarization). Skipping DIKW check.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 5: Interactive Engine & Traversal\n",
    "logger.info(\"--- Scenario 5: Interactive Traversal ---\")\n",
    "\n",
    "interactive_engine = InteractiveRaptorEngine(store=store, summarizer=summarizer, config=config)\n",
    "\n",
    "# Traverse children of root\n",
    "if isinstance(root, SummaryNode):\n",
    "    children = list(interactive_engine.get_children(root))\n",
    "    logger.info(f\"Root has {len(children)} children.\")\n",
    "\n",
    "    # Traverse grandchildren\n",
    "    if children and isinstance(children[0], SummaryNode):\n",
    "        grandchildren = list(interactive_engine.get_children(children[0]))\n",
    "        logger.info(f\"First child has {len(grandchildren)} children.\")\n",
    "    else:\n",
    "        logger.info(\"First child is a Leaf Chunk.\")\n",
    "        grandchildren = []\n",
    "else:\n",
    "    children = []\n",
    "    grandchildren = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 6: Refinement\n",
    "logger.info(\"--- Scenario 6: Interactive Refinement ---\")\n",
    "\n",
    "# Pick a node to refine (preferably a SummaryNode)\n",
    "target_node = None\n",
    "for child in children:\n",
    "    if isinstance(child, SummaryNode):\n",
    "        target_node = child\n",
    "        break\n",
    "\n",
    "if target_node:\n",
    "    logger.info(f\"Refining Node: {target_node.id}\")\n",
    "    logger.info(f\"Original Text: {target_node.text[:50]}...\")\n",
    "\n",
    "    refined_node = interactive_engine.refine_node(target_node.id, \"Explain simply.\")\n",
    "\n",
    "    logger.info(f\"Refined Text: {refined_node.text[:50]}...\")\n",
    "    assert refined_node.metadata.is_user_edited is True\n",
    "    assert \"Explain simply.\" in refined_node.metadata.refinement_history\n",
    "    logger.info(\"‚úÖ Node Refinement Validated\")\n",
    "else:\n",
    "    logger.warning(\"No SummaryNode children found to refine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emfo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 7: Traceability\n",
    "logger.info(\"--- Scenario 7: Traceability (Source Chunks) ---\")\n",
    "\n",
    "if target_node:\n",
    "    source_chunks = list(interactive_engine.get_source_chunks(target_node.id, limit=5))\n",
    "    logger.info(f\"Retrieved {len(source_chunks)} source chunks for node {target_node.id}\")\n",
    "\n",
    "    for sc in source_chunks:\n",
    "        logger.info(f\" - Chunk {sc.index}: {sc.text[:30]}...\")\n",
    "\n",
    "    assert len(source_chunks) > 0\n",
    "    logger.info(\"‚úÖ Source Chunk Retrieval Validated\")\n",
    "else:\n",
    "    logger.warning(\"Skipping traceability check (no target node).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 8: Markdown Export\n",
    "logger.info(\"--- Scenario 8: Export ---\")\n",
    "\n",
    "md_content = export_to_markdown(tree, store)\n",
    "\n",
    "output_md_path = project_root / \"summary_all.md\"\n",
    "with open(output_md_path, \"w\", encoding=\"utf-8\") as _f_md:\n",
    "    _f_md.write(md_content)\n",
    "logger.info(f\"Exported Markdown to {output_md_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nWHF",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 8 Continued: Canvas Export\n",
    "canvas_exporter = ObsidianCanvasExporter()\n",
    "\n",
    "output_canvas_path = project_root / \"summary_kj.canvas\"\n",
    "\n",
    "# Use export method which writes to file\n",
    "canvas_exporter.export(tree, output_canvas_path, store)\n",
    "\n",
    "logger.info(f\"Exported Canvas to {output_canvas_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 9: GUI Instructions\n",
    "logger.info(\"--- Scenario 9: GUI Launch Instructions ---\")\n",
    "print(\"\\nTo explore the results visually, run the following command in your terminal:\")\n",
    "print(\"uv run matome serve tutorials/chunks.db\")\n",
    "print(\"\\nThis will launch the Panel-based interactive application.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZHCJ",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "store.close()\n",
    "for _cm in context_managers:\n",
    "    _cm.stop()\n",
    "logger.info(\"Mocks deactivated.\")\n",
    "logger.info(\"üéâ All Systems Go: Matome 2.0 is ready for Knowledge Installation.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "marimo": {
   "marimo_version": "0.19.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
