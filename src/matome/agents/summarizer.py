"""
Summarization Agent module.
This module implements the summarization logic using OpenRouter and Chain of Density prompting.
"""

import json
import logging
import re
import unicodedata
import uuid
from pathlib import Path
from typing import Any

from langchain_core.messages import BaseMessage, HumanMessage
from langchain_openai import ChatOpenAI
from tenacity import Retrying, stop_after_attempt, wait_exponential

from domain_models.config import ProcessingConfig
from domain_models.constants import PROMPT_INJECTION_PATTERNS
from domain_models.data_schema import DIKWLevel, NodeMetadata
from domain_models.manifest import SummaryNode
from matome.config import get_openrouter_api_key, get_openrouter_base_url
from matome.exceptions import SummarizationError
from matome.interfaces import PromptStrategy

logger = logging.getLogger(__name__)


class SummarizationAgent:
    """
    Agent responsible for summarizing text using an LLM.
    """

    def __init__(
        self,
        config: ProcessingConfig,
        strategy: PromptStrategy,
        llm: ChatOpenAI | None = None,
    ) -> None:
        """
        Initialize the SummarizationAgent.

        Args:
            config: Processing configuration containing model name, retries, etc.
            strategy: The prompt strategy to use for summarization.
            llm: Optional pre-configured LLM instance. If None, it will be initialized from config.
        """
        self.config = config
        self.strategy = strategy
        self.model_name = config.summarization_model

        # Determine API key and Base URL
        api_key = get_openrouter_api_key()
        base_url = get_openrouter_base_url()

        self.mock_mode = api_key == "mock"

        self.llm: ChatOpenAI | None = None

        if llm:
            self.llm = llm
        elif api_key and not self.mock_mode:
            self.llm = ChatOpenAI(
                model=self.model_name,
                api_key=api_key,
                base_url=base_url,
                temperature=config.llm_temperature,
                max_retries=config.max_retries,
            )
        else:
            self.llm = None

    def summarize(
        self,
        text: str | list[str],
        context: dict[str, Any] | None = None,
        strategy: PromptStrategy | None = None,
    ) -> SummaryNode:
        """
        Summarize the provided text using the injected strategy.

        Args:
            text: The text to summarize. Can be a string or list of strings.
            context: Optional context containing metadata (e.g., node ID, level).
            strategy: Optional strategy override for this specific call. If None,
                      the default strategy provided at initialization is used.

        Returns:
            The summary node generated by the model.
        """
        request_id = str(uuid.uuid4())
        active_strategy = strategy or self.strategy

        # Process input text (for logging/validation, we join if list)
        text_str = "\n\n".join(text) if isinstance(text, list) else text

        if not text_str:
            logger.debug(f"[{request_id}] Skipping empty text summarization.")
            default_meta = NodeMetadata(dikw_level=DIKWLevel.DATA)
            if context:
                ctx = context.copy()
                # If context does not specify metadata, use default.
                # If it does, we trust it (validating at SummaryNode construction).
                if "metadata" not in ctx:
                    ctx["metadata"] = default_meta

                return SummaryNode(text="", **ctx)
            return SummaryNode(
                id=str(uuid.uuid4()),
                text="",
                level=0,
                children_indices=[],
                metadata=default_meta,
            )

        # Validate input for security
        self._validate_input(text_str, self.config.max_input_length, self.config.max_word_length)

        # Sanitize prompt injection
        if isinstance(text, list):
            safe_text: str | list[str] = [self._sanitize_prompt_injection(t) for t in text]
            safe_text_str = "\n".join(safe_text)
        else:
            safe_text = self._sanitize_prompt_injection(text)
            safe_text_str = safe_text

        if self.mock_mode:
            return self._handle_mock_mode(safe_text_str, context, request_id, active_strategy)

        if not self.llm:
            msg = f"[{request_id}] LLM not initialized (missing API Key?). Cannot perform summarization."
            logger.error(msg)
            raise SummarizationError(msg)

        if self.config.summarization_model != self.model_name:
            logger.debug(
                f"[{request_id}] Config model {self.config.summarization_model} differs from agent model {self.model_name}. Using agent model."
            )

        try:
            prompt = active_strategy.format_prompt(safe_text, context)
            messages = [HumanMessage(content=prompt)]

            response = self._invoke_llm(messages, self.config, request_id)
            response_content = self._process_response(response, request_id)

            return self._create_summary_node(response_content, context, active_strategy)

        except Exception as e:
            logger.exception(f"[{request_id}] Summarization failed for text length {len(text_str)}")
            msg = f"Summarization failed: {e}"
            raise SummarizationError(msg) from e

    def _handle_mock_mode(
        self,
        safe_text_str: str,
        context: dict[str, Any] | None,
        request_id: str,
        strategy: PromptStrategy | None = None,
    ) -> SummaryNode:
        """
        Handle summarization when in mock mode.

        Returns a static summary without calling an external LLM, useful for testing
        and development without incurring API costs.

        Args:
            safe_text_str: The sanitized input text.
            context: Optional context dictionary.
            request_id: Unique request identifier.
            strategy: The active strategy used for this request.

        Returns:
            A SummaryNode with mock content.
        """
        logger.info(f"[{request_id}] Mock mode enabled. Returning static summary.")

        # Determine strategy name to look up mock file
        strategy_name = "default"
        if strategy:
            if hasattr(strategy, "base_strategy"):
                # Handle wrapper strategies (e.g., RefinementStrategy)
                strategy_name = "refinement"
            else:
                # Handle normal strategies (WisdomStrategy, etc.)
                strategy_name = type(strategy).__name__.lower().replace("strategy", "")

        # Try to load mock response from file
        mock_file = (
            Path(__file__).parents[3] / "tests" / "data" / "mock_responses" / f"{strategy_name}.json"
        )

        if mock_file.exists():
            try:
                with open(mock_file, encoding="utf-8") as f:
                    data = json.load(f)

                # Ensure we have a SummaryNode compatible dict
                if "text" not in data and "summary" in data:
                    data["text"] = data.pop("summary")

                # Handle metadata
                if "metadata" in data and isinstance(data["metadata"], dict):
                    # Keep as dict for now, will be merged/converted later if needed
                    pass
                else:
                    data["metadata"] = {}

                # Merge with context
                if context:
                    ctx_meta = context.get("metadata")
                    if ctx_meta and isinstance(ctx_meta, dict):
                        data["metadata"].update(ctx_meta)
                    data.update({k: v for k, v in context.items() if k != "metadata"})

                # Convert metadata dict to NodeMetadata if not already
                if isinstance(data.get("metadata"), dict):
                    # Ensure defaults if missing
                    meta_dict = data["metadata"]
                    if "dikw_level" not in meta_dict:
                        meta_dict["dikw_level"] = DIKWLevel.DATA
                    data["metadata"] = NodeMetadata(**meta_dict)

                # Ensure required fields
                if "id" not in data:
                    data["id"] = str(uuid.uuid4())
                if "level" not in data:
                    data["level"] = 1
                if "children_indices" not in data:
                    data["children_indices"] = []

                return SummaryNode(**data)

            except Exception as e:
                logger.warning(f"Failed to load mock response from {mock_file}: {e}")
                # Fallback to default behavior below

        # Default fallback behavior (preserves backward compatibility)
        mock_summary = f"Summary of {safe_text_str[:20]}..."

        # Default metadata
        meta = NodeMetadata(dikw_level=DIKWLevel.DATA, is_user_edited=False)

        data = {"text": mock_summary, "metadata": meta}

        if context:
            # Handle metadata merge manually for mock mode too
            ctx_meta = context.get("metadata")
            if ctx_meta and isinstance(ctx_meta, dict):
                for k, v in ctx_meta.items():
                    setattr(meta, k, v)

                context_copy = context.copy()
                context_copy.pop("metadata")
                data.update(context_copy)
            else:
                data.update(context)

        # Ensure required fields are present if not in context
        if "id" not in data:
            data["id"] = str(uuid.uuid4())
        if "level" not in data:
            data["level"] = 1
        if "children_indices" not in data:
            data["children_indices"] = []

        return SummaryNode(**data)

    def _create_summary_node(
        self,
        response_content: str,
        context: dict[str, Any] | None,
        strategy: PromptStrategy,
    ) -> SummaryNode:
        """
        Create a SummaryNode from the LLM response.

        Parses the response using the strategy, merges it with provided context,
        and constructs the Pydantic model.

        Args:
            response_content: The raw text response from the LLM.
            context: Optional context dictionary to merge.
            strategy: The strategy used for parsing.

        Returns:
            A validated SummaryNode object.
        """
        parsed_data = strategy.parse_output(response_content)

        self._normalize_parsed_data(parsed_data)

        if context:
            self._merge_context(parsed_data, context)

        return SummaryNode(**parsed_data)

    def _normalize_parsed_data(self, parsed_data: dict[str, Any]) -> None:
        """Normalize parsed data keys and ensure metadata object."""
        # Map 'summary' to 'text' if needed
        if "summary" in parsed_data and "text" not in parsed_data:
            parsed_data["text"] = parsed_data.pop("summary")

        # Ensure metadata exists as NodeMetadata object
        if "metadata" not in parsed_data:
            parsed_data["metadata"] = NodeMetadata(dikw_level=DIKWLevel.DATA)
        elif isinstance(parsed_data["metadata"], dict):
            # Try to upgrade dict to NodeMetadata if it has dikw_level, else default
            meta_dict = parsed_data["metadata"]
            if "dikw_level" not in meta_dict:
                meta_dict["dikw_level"] = DIKWLevel.DATA
            parsed_data["metadata"] = NodeMetadata(**meta_dict)

    def _merge_context(self, parsed_data: dict[str, Any], context: dict[str, Any]) -> None:
        """Smart merge of context into parsed data."""
        strat_meta = parsed_data.get("metadata")
        ctx_meta = context.get("metadata")

        # 1. Merge Metadata
        if strat_meta and isinstance(strat_meta, NodeMetadata) and isinstance(ctx_meta, dict):
            # Update NodeMetadata object with context metadata (e.g. cluster_id)
            # Only update fields that exist in NodeMetadata
            for k, v in ctx_meta.items():
                if k in NodeMetadata.model_fields:
                    setattr(strat_meta, k, v)

        # 2. Merge Top-level fields
        # Only merge fields that are part of SummaryNode schema
        summary_node_fields = SummaryNode.model_fields.keys()
        for k, v in context.items():
            if k in summary_node_fields and k != "metadata":
                parsed_data[k] = v

    def _validate_input(self, text: str, max_input_length: int, max_word_length: int) -> None:
        """
        Sanitize and validate input text.
        """
        # 0. Unicode Normalization (NFKC)
        normalized_text = unicodedata.normalize("NFKC", text)

        # 1. Length Check
        if len(normalized_text) > max_input_length:
            msg = f"Input text exceeds maximum allowed length ({max_input_length} characters)."
            raise ValueError(msg)

        # 2. Control Character Check (Unicode)
        # Allowed whitespace and typical formatting characters
        allowed_controls = {"\n", "\t", "\r"}

        for char in normalized_text:
            category = unicodedata.category(char)
            # Check for control categories: Cc (Control), Cf (Format), Co (Private Use), Cn (Not Assigned)
            # Cs (Surrogate) - Python strings usually don't have Cs if valid unicode, but good to check.
            # Emojis are typically So (Symbol, Other), which is fine.
            if category in ("Cc", "Co", "Cn") and char not in allowed_controls:
                # We strictly disallow Cc (Control) except whitespace.
                # We allow Cf (Format) because it includes ZWJ/ZWNJ often used in emojis and text layout.
                msg = f"Input text contains invalid control character: {char!r} (U+{ord(char):04X})"
                raise ValueError(msg)

        # 3. Tokenizer DoS Protection
        words = normalized_text.split()
        if not words:
            return

        longest_word_len = max((len(w) for w in words), default=0)
        if longest_word_len > max_word_length:
            msg = f"Input text contains extremely long words (>{max_word_length} chars) - potential DoS vector."
            raise ValueError(msg)

    def _sanitize_prompt_injection(self, text: str) -> str:
        """
        Basic mitigation for Prompt Injection using pattern matching.

        This implementation uses regular expressions to detect and redact known
        prompt injection patterns (e.g., "ignore previous instructions").
        While not foolproof against sophisticated adversarial attacks, it provides
        a baseline level of defense against common injection attempts.

        Future improvements could include:
        - Integrating a dedicated security model (e.g., Llama Guard).
        - Using an external sanitization library.
        """
        # 1. Normalize text first (NFKC)
        normalized_text = unicodedata.normalize("NFKC", text)

        sanitized = normalized_text
        for pattern in PROMPT_INJECTION_PATTERNS:
            # We use re.sub for flexible matching with IGNORECASE which handles case folding.
            # The patterns in PROMPT_INJECTION_PATTERNS are assumed to be raw regex strings.
            sanitized = re.sub(pattern, "[Filtered]", sanitized, flags=re.IGNORECASE)

        return sanitized

    def _invoke_llm(
        self, messages: list[HumanMessage], config: ProcessingConfig, request_id: str
    ) -> BaseMessage:
        """
        Invoke the LLM with exponential backoff retry logic.

        Args:
            messages: List of messages to send to the LLM.
            config: Processing configuration (for max retries).
            request_id: Unique identifier for logging.

        Returns:
            The raw response message from the LLM.

        Raises:
            SummarizationError: If LLM is not initialized or call fails after retries.
        """
        if not self.llm:
            msg = "LLM not initialized"
            raise SummarizationError(msg)

        response = None
        for attempt in Retrying(
            stop=stop_after_attempt(config.max_retries),
            wait=wait_exponential(multiplier=1, min=2, max=10),
            reraise=True,
        ):
            with attempt:
                if attempt.retry_state.attempt_number > 1:
                    logger.warning(
                        f"[{request_id}] Retrying LLM call (Attempt {attempt.retry_state.attempt_number}/{config.max_retries})"
                    )

                if hasattr(self.llm, "invoke"):
                    response = self.llm.invoke(messages)
                else:
                    response = self.llm(messages)  # type: ignore[operator]

        if not response:
            msg = f"[{request_id}] No response received from LLM."
            raise SummarizationError(msg)

        return response

    def _process_response(self, response: BaseMessage, request_id: str) -> str:
        """
        Process and extract content from the LLM response.
        """
        content: str | list[str | dict[str, Any]] = response.content

        if isinstance(content, str):
            return content
        if isinstance(content, list):
            logger.warning(f"[{request_id}] Received list content from LLM: {content}")
            return " ".join([str(c) for c in content])

        logger.warning(f"[{request_id}] Received unexpected content type from LLM: {type(content)}")
        return str(content)
