from collections.abc import Iterable
from typing import Any, Protocol, runtime_checkable

from domain_models.config import ProcessingConfig
from domain_models.manifest import Chunk, Cluster, SummaryNode


@runtime_checkable
class Chunker(Protocol):
    """
    Protocol for text chunking engines.

    Implementations are responsible for dividing large text documents into smaller,
    manageable segments (chunks) while preserving semantic meaning as much as possible.
    """

    def split_text(self, text: str, config: ProcessingConfig) -> Iterable[Chunk]:
        """
        Split text into chunks.

        This method should handle normalization, sentence splitting, and merging based on
        the configuration provided (e.g., max_tokens).

        Args:
            text: The full input text to be processed. If empty, should yield nothing.
            config: Configuration parameters including `max_tokens` and `overlap`.

        Yields:
            `Chunk` objects, each containing the chunk text and metadata.
            Yields nothing if the input text is empty.
        """
        ...


@runtime_checkable
class Clusterer(Protocol):
    """
    Protocol for clustering engines.

    Implementations should group similar text chunks or nodes based on their vector embeddings
    to facilitate hierarchical summarization (RAPTOR).
    """

    def cluster_nodes(
        self, embeddings: Iterable[list[float]], config: ProcessingConfig
    ) -> list[Cluster]:
        """
        Cluster nodes based on embeddings.

        This method identifies groups of similar nodes to be summarized together.

        Args:
            embeddings: An iterable of vectors (list of floats), where each vector corresponds to a node.
                        The order of embeddings implies the index (0..N-1).
            config: Configuration parameters such as `n_clusters` or `clustering_algorithm`.

        Returns:
            A list of `Cluster` objects.
            Each `Cluster` contains `node_indices` which correspond to the indices (0..N-1) of the
            input `embeddings` list.
        """
        ...


@runtime_checkable
class PromptStrategy(Protocol):
    """
    Interface for summarization strategies.
    Implementations must define how to construct prompts and parse responses.
    """

    def format_prompt(
        self, text: str | list[str], context: dict[str, Any] | None = None
    ) -> str:
        """
        Constructs the prompt string from the input text and optional context.
        """
        ...

    def parse_output(self, response: str) -> dict[str, Any]:
        """
        Parses the raw LLM response string into a structured dictionary.
        Must return at least {'summary': str}.
        """
        ...


@runtime_checkable
class Summarizer(Protocol):
    """
    Protocol for summarization engines.

    Implementations should generate concise summaries of the provided text, respecting
    token limits and other configuration parameters.
    """

    def summarize(
        self, text: str | list[str], context: dict[str, Any] | None = None
    ) -> SummaryNode:
        """
        Summarize the provided text.

        Args:
            text: The text to summarize. Can be a string or list of strings.
            context: Optional context containing metadata (e.g., node ID, level).

        Returns:
            The summary node generated by the model.
        """
        ...
